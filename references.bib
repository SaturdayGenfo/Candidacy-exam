
@inproceedings{heusel_gans_2017,
	title = {{GANs} {Trained} by a {Two} {Time}-{Scale} {Update} {Rule} {Converge} to a {Local} {Nash} {Equilibrium}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/8a1d694707eb0fefe65871369074926d-Abstract.html},
	urldate = {2021-08-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
	year = {2017},
}

@article{vincent_connection_2011,
	title = {A {Connection} {Between} {Score} {Matching} and {Denoising} {Autoencoders}},
	volume = {23},
	issn = {0899-7667},
	doi = {10.1162/NECO_a_00142},
	abstract = {Denoising autoencoders have been previously shown to be competitive alternatives to restricted Boltzmann machines for unsupervised pretraining of each layer of a deep architecture. We show that a simple denoising autoencoder training criterion is equivalent to matching the score (with respect to the data) of a specific energy-based model to that of a nonparametric Parzen density estimator of the data. This yields several useful insights. It defines a proper probabilistic model for the denoising autoencoder technique, which makes it in principle possible to sample from them or rank examples by their energy. It suggests a different way to apply score matching that is related to learning to denoise and does not require computing second derivatives. It justifies the use of tied weights between the encoder and decoder and suggests ways to extend the success of denoising autoencoders to a larger family of energy-based models.},
	number = {7},
	journal = {Neural Computation},
	author = {Vincent, Pascal},
	month = jul,
	year = {2011},
	note = {Conference Name: Neural Computation},
	pages = {1661--1674},
}

@inproceedings{song_sliced_2020,
	title = {Sliced {Score} {Matching}: {A} {Scalable} {Approach} to {Density} and {Score} {Estimation}},
	shorttitle = {Sliced {Score} {Matching}},
	url = {http://proceedings.mlr.press/v115/song20a.html},
	language = {en},
	urldate = {2021-08-13},
	booktitle = {Uncertainty in {Artificial} {Intelligence}},
	publisher = {PMLR},
	author = {Song, Yang and Garg, Sahaj and Shi, Jiaxin and Ermon, Stefano},
	month = aug,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {574--584},
}

@article{hyvarinen_estimation_2005,
	title = {Estimation of {Non}-{Normalized} {Statistical} {Models} by {Score} {Matching}},
	volume = {6},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v6/hyvarinen05a.html},
	abstract = {One often wants to estimate statistical models where the probability density function is known only up to a multiplicative normalization constant. Typically, one then has to resort to Markov Chain Monte Carlo methods, or approximations of the normalization constant. Here, we propose that such models can be estimated by minimizing the expected squared distance between the gradient of the log-density given by the model and the gradient of the log-density of the observed data. While the estimation of the gradient of log-density function is, in principle, a very difficult non-parametric problem, we prove a surprising result that gives a simple formula for this objective function. The density function of the observed data does not appear in this formula, which simplifies to a sample average of a sum of some derivatives of the log-density given by the model. The validity of the method is demonstrated on multivariate Gaussian and independent component analysis models, and by estimating an overcomplete filter set for natural image data.},
	number = {24},
	urldate = {2021-08-13},
	journal = {Journal of Machine Learning Research},
	author = {Hyvärinen, Aapo},
	year = {2005},
	pages = {695--709},
}

@article{hyvarinen_estimation_nodate,
	title = {Estimation of {Non}-{Normalized} {Statistical} {Models} by {Score} {Matching}},
	abstract = {One often wants to estimate statistical models where the probability density function is known only up to a multiplicative normalization constant. Typically, one then has to resort to Markov Chain Monte Carlo methods, or approximations of the normalization constant. Here, we propose that such models can be estimated by minimizing the expected squared distance between the gradient of the log-density given by the model and the gradient of the log-density of the observed data. While the estimation of the gradient of log-density function is, in principle, a very difﬁcult non-parametric problem, we prove a surprising result that gives a simple formula for this objective function. The density function of the observed data does not appear in this formula, which simpliﬁes to a sample average of a sum of some derivatives of the log-density given by the model. The validity of the method is demonstrated on multivariate Gaussian and independent component analysis models, and by estimating an overcomplete ﬁlter set for natural image data.},
	language = {en},
	author = {Hyvarinen, Aapo},
	pages = {14},
}

@inproceedings{vempala_rapid_2019,
	title = {Rapid {Convergence} of the {Unadjusted} {Langevin} {Algorithm}: {Isoperimetry} {Suffices}},
	volume = {32},
	shorttitle = {Rapid {Convergence} of the {Unadjusted} {Langevin} {Algorithm}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/65a99bb7a3115fdede20da98b08a370f-Abstract.html},
	urldate = {2021-08-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vempala, Santosh and Wibisono, Andre},
	year = {2019},
}

@inproceedings{song_generative_2019,
	title = {Generative {Modeling} by {Estimating} {Gradients} of the {Data} {Distribution}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/3001ef257407d5a371a96dcd947c7d93-Abstract.html},
	urldate = {2021-08-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Song, Yang and Ermon, Stefano},
	year = {2019},
}

@article{song_generative_2019-1,
	title = {Generative {Modeling} by {Estimating} {Gradients} of the {Data} {Distribution}},
	url = {https://openreview.net/forum?id=B1lcYrBgLH},
	abstract = {We explore a new class of generative models based on estimating the vector field of gradients of the data distribution using score matching, and employing Langevin dynamics to generate samples. To...},
	language = {en},
	urldate = {2021-08-13},
	author = {Song, Yang and Ermon, Stefano},
	month = sep,
	year = {2019},
}

@inproceedings{tzen_local_2018,
	title = {Local {Optimality} and {Generalization} {Guarantees} for the {Langevin} {Algorithm} via {Empirical} {Metastability}},
	url = {http://proceedings.mlr.press/v75/tzen18a.html},
	language = {en},
	urldate = {2021-08-13},
	booktitle = {Conference {On} {Learning} {Theory}},
	publisher = {PMLR},
	author = {Tzen, Belinda and Liang, Tengyuan and Raginsky, Maxim},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {857--875},
}

@article{vempala_rapid_2019-1,
	title = {Rapid {Convergence} of the {Unadjusted} {Langevin} {Algorithm}: {Isoperimetry} {Suffices}},
	shorttitle = {Rapid {Convergence} of the {Unadjusted} {Langevin} {Algorithm}},
	url = {http://arxiv.org/abs/1903.08568},
	abstract = {We study the Unadjusted Langevin Algorithm (ULA) for sampling from a probability distribution \${\textbackslash}nu = e{\textasciicircum}\{-f\}\$ on \${\textbackslash}mathbb\{R\}{\textasciicircum}n\$. We prove a convergence guarantee in Kullback-Leibler (KL) divergence assuming \${\textbackslash}nu\$ satisfies a log-Sobolev inequality and the Hessian of \$f\$ is bounded. Notably, we do not assume convexity or bounds on higher derivatives. We also prove convergence guarantees in R{\textbackslash}'enyi divergence of order \$q {\textgreater} 1\$ assuming the limit of ULA satisfies either the log-Sobolev or Poincar{\textbackslash}'e inequality.},
	urldate = {2020-08-04},
	journal = {arXiv:1903.08568 [cs, math, stat]},
	author = {Vempala, Santosh S. and Wibisono, Andre},
	month = aug,
	year = {2019},
	note = {arXiv: 1903.08568},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning, Mathematics - Probability, Statistics - Machine Learning},
}

@article{song_generative_2020,
	title = {Generative {Modeling} by {Estimating} {Gradients} of the {Data} {Distribution}},
	url = {http://arxiv.org/abs/1907.05600},
	abstract = {We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.},
	urldate = {2021-08-13},
	journal = {arXiv:1907.05600 [cs, stat]},
	author = {Song, Yang and Ermon, Stefano},
	month = oct,
	year = {2020},
	note = {arXiv: 1907.05600},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{barratt_note_2018,
	title = {A {Note} on the {Inception} {Score}},
	url = {http://arxiv.org/abs/1801.01973},
	abstract = {Deep generative models are powerful tools that have produced impressive results in recent years. These advances have been for the most part empirically driven, making it essential that we use high quality evaluation metrics. In this paper, we provide new insights into the Inception Score, a recently proposed and widely used evaluation metric for generative models, and demonstrate that it fails to provide useful guidance when comparing models. We discuss both suboptimalities of the metric itself and issues with its application. Finally, we call for researchers to be more systematic and careful when evaluating and comparing generative models, as the advancement of the field depends upon it.},
	urldate = {2021-08-12},
	journal = {arXiv:1801.01973 [cs, stat]},
	author = {Barratt, Shane and Sharma, Rishi},
	month = jun,
	year = {2018},
	note = {arXiv: 1801.01973},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{yaida_fluctuation-dissipation_2018,
	title = {Fluctuation-dissipation relations for stochastic gradient descent},
	url = {http://arxiv.org/abs/1810.00004},
	abstract = {The notion of the stationary equilibrium ensemble has played a central role in statistical mechanics. In machine learning as well, training serves as generalized equilibration that drives the probability distribution of model parameters toward stationarity. Here, we derive stationary fluctuation-dissipation relations that link measurable quantities and hyperparameters in the stochastic gradient descent algorithm. These relations hold exactly for any stationary state and can in particular be used to adaptively set training schedule. We can further use the relations to efficiently extract information pertaining to a loss-function landscape such as the magnitudes of its Hessian and anharmonicity. Our claims are empirically verified.},
	urldate = {2021-08-07},
	journal = {arXiv:1810.00004 [cs, stat]},
	author = {Yaida, Sho},
	month = dec,
	year = {2018},
	note = {arXiv: 1810.00004},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{ziyin_sgd_2021,
	title = {{SGD} {May} {Never} {Escape} {Saddle} {Points}},
	url = {http://arxiv.org/abs/2107.11774},
	abstract = {Stochastic gradient descent (SGD) has been deployed to solve highly non-linear and non-convex machine learning problems such as the training of deep neural networks. However, previous works on SGD often rely on highly restrictive and unrealistic assumptions about the nature of noise in SGD. In this work, we mathematically construct examples that defy previous understandings of SGD. For example, our constructions show that: (1) SGD may converge to a local maximum; (2) SGD may escape a saddle point arbitrarily slowly; (3) SGD may prefer sharp minima over the flat ones; and (4) AMSGrad may converge to a local maximum. Our result suggests that the noise structure of SGD might be more important than the loss landscape in neural network training and that future research should focus on deriving the actual noise structure in deep learning.},
	urldate = {2021-08-03},
	journal = {arXiv:2107.11774 [cs, math, stat]},
	author = {Ziyin, Liu and Li, Botao and Ueda, Masahito},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.11774},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{zhu_anisotropic_2019,
	title = {The {Anisotropic} {Noise} in {Stochastic} {Gradient} {Descent}: {Its} {Behavior} of {Escaping} from {Sharp} {Minima} and {Regularization} {Effects}},
	shorttitle = {The {Anisotropic} {Noise} in {Stochastic} {Gradient} {Descent}},
	url = {http://arxiv.org/abs/1803.00195},
	abstract = {Understanding the behavior of stochastic gradient descent (SGD) in the context of deep neural networks has raised lots of concerns recently. Along this line, we study a general form of gradient based optimization dynamics with unbiased noise, which unifies SGD and standard Langevin dynamics. Through investigating this general optimization dynamics, we analyze the behavior of SGD on escaping from minima and its regularization effects. A novel indicator is derived to characterize the efficiency of escaping from minima through measuring the alignment of noise covariance and the curvature of loss function. Based on this indicator, two conditions are established to show which type of noise structure is superior to isotropic noise in term of escaping efficiency. We further show that the anisotropic noise in SGD satisfies the two conditions, and thus helps to escape from sharp and poor minima effectively, towards more stable and flat minima that typically generalize well. We systematically design various experiments to verify the benefits of the anisotropic noise, compared with full gradient descent plus isotropic diffusion (i.e. Langevin dynamics).},
	urldate = {2021-08-03},
	journal = {arXiv:1803.00195 [cs, stat]},
	author = {Zhu, Zhanxing and Wu, Jingfeng and Yu, Bing and Wu, Lei and Ma, Jinwen},
	month = jun,
	year = {2019},
	note = {arXiv: 1803.00195},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{noauthor_mail_nodate,
	title = {Mail - leello.dadi@epfl.ch},
	url = {https://ewa.epfl.ch/owa/#path=/mail},
	urldate = {2021-04-22},
}

@misc{noauthor_about_2013,
	title = {About / {Top} {Posts}},
	url = {https://slatestarcodex.com/about/},
	abstract = {Welcome to Slate Star Codex, a blog about science, medicine, philosophy, politics, and futurism. (there’s also one post about hallucinatory cactus-people, but it’s not representative) S…},
	language = {en-US},
	urldate = {2021-04-05},
	journal = {Slate Star Codex},
	month = jan,
	year = {2013},
}

@article{donoho_uncertainty_1989,
	title = {Uncertainty {Principles} and {Signal} {Recovery}},
	volume = {49},
	issn = {0036-1399},
	url = {https://epubs.siam.org/doi/10.1137/0149053},
	doi = {10.1137/0149053},
	abstract = {The uncertainty principle can easily be generalized to cases where the “sets of concentration” are not intervals. Such generalizations are presented for continuous and discrete-time functions, and for several measures of “concentration” (e.g., \$L\_2 \$ and \$L\_1 \$ measures). The generalizations explain interesting phenomena in signal recovery problems where there is an interplay of missing data, sparsity, and bandlimiting.},
	number = {3},
	urldate = {2021-04-01},
	journal = {SIAM Journal on Applied Mathematics},
	author = {Donoho, David L. and Stark, Philip B.},
	month = jun,
	year = {1989},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {906--931},
}

@article{amari_when_2020,
	title = {When {Does} {Preconditioning} {Help} or {Hurt} {Generalization}?},
	url = {http://arxiv.org/abs/2006.10732},
	abstract = {While second order optimizers such as natural gradient descent (NGD) often speed up optimization, their effect on generalization has been called into question. This work presents a more nuanced view on how the {\textbackslash}textit\{implicit bias\} of first- and second-order methods affects the comparison of generalization properties. We provide an exact asymptotic bias-variance decomposition of the generalization error of overparameterized ridgeless regression under a general class of preconditioner \${\textbackslash}boldsymbol\{P\}\$, and consider the inverse population Fisher information matrix (used in NGD) as a particular example. We determine the optimal \${\textbackslash}boldsymbol\{P\}\$ for both the bias and variance, and find that the relative generalization performance of different optimizers depends on the label noise and the "shape" of the signal (true parameters): when the labels are noisy, the model is misspecified, or the signal is misaligned with the features, NGD can achieve lower risk; conversely, GD generalizes better than NGD under clean labels, a well-specified model, or aligned signal. Based on this analysis, we discuss several approaches to manage the bias-variance tradeoff, and the potential benefit of interpolating between GD and NGD. We then extend our analysis to regression in the reproducing kernel Hilbert space and demonstrate that preconditioned GD can decrease the population risk faster than GD. Lastly, we empirically compare the generalization error of first- and second-order optimizers in neural network experiments, and observe robust trends matching our theoretical analysis.},
	urldate = {2021-03-28},
	journal = {arXiv:2006.10732 [cs, stat]},
	author = {Amari, Shun-ichi and Ba, Jimmy and Grosse, Roger and Li, Xuechen and Nitanda, Atsushi and Suzuki, Taiji and Wu, Denny and Xu, Ji},
	month = dec,
	year = {2020},
	note = {arXiv: 2006.10732},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{anil_sorting_2019,
	title = {Sorting {Out} {Lipschitz} {Function} {Approximation}},
	url = {http://proceedings.mlr.press/v97/anil19a.html},
	abstract = {Training neural networks under a strict Lipschitz constraint is useful for provable adversarial robustness, generalization bounds, interpretable gradients, and Wasserstein distance estimation. By t...},
	language = {en},
	urldate = {2021-03-28},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Anil, Cem and Lucas, James and Grosse, Roger},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {291--301},
}

@article{chen_convergence_2015,
	title = {On the {Convergence} of {Stochastic} {Gradient} {MCMC} {Algorithms} with {High}-{Order} {Integrators}},
	volume = {28},
	url = {https://proceedings.neurips.cc/paper/2015/hash/af4732711661056eadbf798ba191272a-Abstract.html},
	language = {en},
	urldate = {2021-02-11},
	journal = {Advances in Neural Information Processing Systems},
	author = {Chen, Changyou and Ding, Nan and Carin, Lawrence},
	year = {2015},
	pages = {2278--2286},
}

@article{roberts_coupling_2007,
	title = {Coupling and {Ergodicity} of {Adaptive} {Markov} {Chain} {Monte} {Carlo} {Algorithms}},
	volume = {44},
	issn = {0021-9002},
	url = {https://www.jstor.org/stable/27595854},
	abstract = {We consider basic ergodicity properties of adaptive Markov chain Monte Carlo algorithms under minimal assumptions, using coupling constructions. We prove convergence in distribution and a weak law of large numbers. We also give counterexamples to demonstrate that the assumptions we make are not redundant.},
	number = {2},
	urldate = {2021-02-11},
	journal = {Journal of Applied Probability},
	author = {Roberts, Gareth O. and Rosenthal, Jeffrey S.},
	year = {2007},
	note = {Publisher: Applied Probability Trust},
	pages = {458--475},
}

@article{haario_adaptive_2001,
	title = {An {Adaptive} {Metropolis} {Algorithm}},
	volume = {7},
	issn = {1350-7265},
	url = {https://www.jstor.org/stable/3318737},
	doi = {10.2307/3318737},
	abstract = {A proper choice of a proposal distribution for Markov chain Monte Carlo methods, for example for the Metropolis-Hastings algorithm, is well known to be a crucial factor for the convergence of the algorithm. In this paper we introduce an adaptive Metropolis (AM) algorithm, where the Gaussian proposal distribution is updated along the process using the full information cumulated so far. Due to the adaptive nature of the process, the AM algorithm is non-Markovian, but we establish here that it has the correct ergodic properties. We also include the results of our numerical tests, which indicate that the AM algorithm competes well with traditional Metropolis-Hastings algorithms, and demonstrate that the AM algorithm is easy to use in practical computation.},
	number = {2},
	urldate = {2021-02-11},
	journal = {Bernoulli},
	author = {Haario, Heikki and Saksman, Eero and Tamminen, Johanna},
	year = {2001},
	note = {Publisher: International Statistical Institute (ISI) and Bernoulli Society for Mathematical Statistics and Probability},
	pages = {223--242},
}

@inproceedings{andrieu_efficiency_2006,
	address = {New York, NY, USA},
	series = {valuetools '06},
	title = {On the efficiency of adaptive {MCMC} algorithms},
	isbn = {978-1-59593-504-5},
	url = {https://doi.org/10.1145/1190095.1190150},
	doi = {10.1145/1190095.1190150},
	abstract = {We study a class of adaptive Markov Chain Monte Carlo (MCMC) processes which aim at behaving as an "optimal" target process via a learning procedure. We show, under appropriate conditions, that the adaptive process and "optimal" (nonadaptive) MCMC algorithm share identical asymptotic properties. The special case of adaptive MCMC algorithms governed by stochastic approximation is considered in details and we apply our results to the adaptive Metropolis algorithm of [1]. We also propose a new class of adaptive MCMC algorithms, called quasi-perfect adaptive MCMC which possesses appealing theoretical and practical properties, as demonstrated through numerical simulations.},
	urldate = {2021-02-11},
	booktitle = {Proceedings of the 1st international conference on {Performance} evaluation methodolgies and tools},
	publisher = {Association for Computing Machinery},
	author = {Andrieu, Christophe and Atchadé, Y. F.},
	month = oct,
	year = {2006},
	keywords = {MCMC, Markov chain Monte Carlo, adaptive Markov chains, coupling, metropolis algorithm, rate of convergence, stochastic approximation},
	pages = {43--es},
}

@article{atchade_adaptive_2006,
	title = {An {Adaptive} {Version} for the {Metropolis} {Adjusted} {Langevin} {Algorithm} with a {Truncated} {Drift}},
	volume = {8},
	issn = {1573-7713},
	url = {https://doi.org/10.1007/s11009-006-8550-0},
	doi = {10.1007/s11009-006-8550-0},
	abstract = {This paper extends some adaptive schemes that have been developed for the Random Walk Metropolis algorithm to more general versions of the Metropolis-Hastings (MH) algorithm, particularly to the Metropolis Adjusted Langevin algorithm of Roberts and Tweedie (1996). Our simulations show that the adaptation drastically improves the performance of such MH algorithms. We study the convergence of the algorithm. Our proves are based on a new approach to the analysis of stochastic approximation algorithms based on mixingales theory.},
	language = {en},
	number = {2},
	urldate = {2021-02-11},
	journal = {Methodology and Computing in Applied Probability},
	author = {Atchadé, Yves F.},
	month = jun,
	year = {2006},
	pages = {235--254},
}

@article{levy_online_2017,
	title = {Online to {Offline} {Conversions}, {Universality} and {Adaptive} {Minibatch} {Sizes}},
	url = {http://arxiv.org/abs/1705.10499},
	abstract = {We present an approach towards convex optimization that relies on a novel scheme which converts online adaptive algorithms into offline methods. In the offline optimization setting, our derived methods are shown to obtain favourable adaptive guarantees which depend on the harmonic sum of the queried gradients. We further show that our methods implicitly adapt to the objective's structure: in the smooth case fast convergence rates are ensured without any prior knowledge of the smoothness parameter, while still maintaining guarantees in the non-smooth setting. Our approach has a natural extension to the stochastic setting, resulting in a lazy version of SGD (stochastic GD), where minibathces are chosen {\textbackslash}emph\{adaptively\} depending on the magnitude of the gradients. Thus providing a principled approach towards choosing minibatch sizes.},
	urldate = {2021-02-11},
	journal = {arXiv:1705.10499 [cs, math, stat]},
	author = {Levy, Kfir Y.},
	month = may,
	year = {2017},
	note = {arXiv: 1705.10499},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{durmus_high-dimensional_2019,
	title = {High-dimensional {Bayesian} inference via the unadjusted {Langevin} algorithm},
	volume = {25},
	issn = {1350-7265},
	url = {https://projecteuclid.org/euclid.bj/1568362045},
	doi = {10.3150/18-BEJ1073},
	abstract = {We consider in this paper the problem of sampling a high-dimensional probability distribution ππ{\textbackslash}pi having a density w.r.t. the Lebesgue measure on ℝdRd{\textbackslash}mathbb\{R\}{\textasciicircum}\{d\}, known up to a normalization constant x↦π(x)=e−U(x)/∫ℝde−U(y)dyx↦π(x)=e−U(x)/∫Rde−U(y)dyx{\textbackslash}mapsto{\textbackslash}pi(x)={\textbackslash}mathrm\{e\}{\textasciicircum}\{-U(x)\}/{\textbackslash}int\_\{{\textbackslash}mathbb\{R\}{\textasciicircum}\{d\}\}{\textbackslash}mathrm\{e\}{\textasciicircum}\{-U(y)\}{\textbackslash},{\textbackslash}mathrm\{d\}y. Such problem naturally occurs for example in Bayesian inference and machine learning. Under the assumption that UUU is continuously differentiable, ∇U∇U{\textbackslash}nabla U is globally Lipschitz and UUU is strongly convex, we obtain non-asymptotic bounds for the convergence to stationarity in Wasserstein distance of order 222 and total variation distance of the sampling method based on the Euler discretization of the Langevin stochastic differential equation, for both constant and decreasing step sizes. The dependence on the dimension of the state space of these bounds is explicit. The convergence of an appropriately weighted empirical measure is also investigated and bounds for the mean square error and exponential deviation inequality are reported for functions which are measurable and bounded. An illustration to Bayesian inference for binary regression is presented to support our claims.},
	language = {EN},
	number = {4A},
	urldate = {2021-02-11},
	journal = {Bernoulli},
	author = {Durmus, Alain and Moulines, Éric},
	month = nov,
	year = {2019},
	mrnumber = {MR4003567},
	zmnumber = {07110114},
	note = {Publisher: Bernoulli Society for Mathematical Statistics and Probability},
	keywords = {Langevin diffusion, Markov chain Monte Carlo, Metropolis adjusted Langevin algorithm, rate of convergence, total variation distance},
	pages = {2854--2882},
}

@article{roberts_exponential_1996,
	title = {Exponential {Convergence} of {Langevin} {Distributions} and {Their} {Discrete} {Approximations}},
	volume = {2},
	issn = {1350-7265},
	url = {https://www.jstor.org/stable/3318418},
	doi = {10.2307/3318418},
	abstract = {In this paper we consider a continuous-time method of approximating a given distribution π using the Langevin diffusion d Lt= d Wt+1/2∇ log π ( Lt) dt. We find conditions under this diffusion converges exponentially quickly to π or does not: in one dimension, these are essentially that for distributions with exponential tails of the form π(x) ∝ exp(-γ {\textbar}x{\textbar}β), 0 {\textless} β {\textless} ∞, exponential convergence occurs if and only if β ≥ 1. We then consider conditions under which the discrete approximations to the diffusion converge. We first show that even when the diffusion itself converges, naive discretizations need not do so. We then consider a 'Metropolis-adjusted' version of the algorithm, and find conditions under which this also converges at an exponential rate: perhaps surprisingly, even the Metropolized version need not converge exponentially fast even if the diffusion does. We briefly discuss a truncated form of the algorithm which, in practice, should avoid the difficulties of the other forms.},
	number = {4},
	urldate = {2021-02-11},
	journal = {Bernoulli},
	author = {Roberts, Gareth O. and Tweedie, Richard L.},
	year = {1996},
	note = {Publisher: International Statistical Institute (ISI) and Bernoulli Society for Mathematical Statistics and Probability},
	pages = {341--363},
}

@article{durmus_analysis_2019,
	title = {Analysis of {Langevin} {Monte} {Carlo} via {Convex} {Optimization}},
	volume = {20},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v20/18-173.html},
	number = {73},
	urldate = {2021-02-11},
	journal = {Journal of Machine Learning Research},
	author = {Durmus, Alain and Majewski, Szymon and Miasojedow, Błażej},
	year = {2019},
	pages = {1--46},
}

@misc{noauthor_analysis_nodate,
	title = {Analysis of {Langevin} {Monte} {Carlo} via {Convex} {Optimization}. - {EPFL}},
	url = {https://slsp-epfl.primo.exlibrisgroup.com},
	abstract = {Analysis of Langevin Monte Carlo via Convex Optimization.-article},
	language = {fr},
	urldate = {2021-02-11},
}

@techreport{dalalyan_theoretical_2014,
	type = {Working {Paper}},
	title = {Theoretical guarantees for approximate sampling from smooth and log-concave densities},
	url = {https://econpapers.repec.org/paper/crswpaper/2014-45.htm},
	abstract = {Sampling from various kinds of distributions is an issue of paramount importance in statistics since it is often the key ingredient for constructing estimators, test procedures or confidence intervals. In many situations, the exact sampling from a given distribution is impossible or computationally expensive and, therefore, one needs to resort to approximate sampling strategies. However, there is no well-developed theory providing meaningful nonasymptotic guarantees for the approximate sampling procedures, especially in the high-dimensional problems. This paper makes some progress in this direction by considering the problem of sampling from a distribution having a smooth and log-concave density defined on Rp, for some integer p {\textgreater} 0. We establish nonasymptotic bounds for the error of approximating the true distribution by the one obtained by the Langevin Monte Carlo method and its variants. We illustrate the effectiveness of the established guarantees with various experiments. Underlying our analysis are insights from the theory of continuous-time diffusion processes, which may be of interest beyond the framework of distributions with log-concave densities considered in the present work.},
	number = {2014-45},
	urldate = {2021-02-11},
	institution = {Center for Research in Economics and Statistics},
	author = {Dalalyan, Arnak},
	month = dec,
	year = {2014},
	keywords = {Approximate sampling, Langevin algorithm, Markov Chain Monte Carlo, Rates of convergence},
}

@inproceedings{simsekli_stochastic_2016,
	title = {Stochastic {Quasi}-{Newton} {Langevin} {Monte} {Carlo}},
	url = {http://proceedings.mlr.press/v48/simsekli16.html},
	abstract = {Recently, Stochastic Gradient Markov Chain Monte Carlo (SG-MCMC) methods have been proposed for scaling up Monte Carlo computations to large data problems. Whilst these approaches have proven usefu...},
	language = {en},
	urldate = {2021-02-11},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Simsekli, Umut and Badeau, Roland and Cemgil, Taylan and Richard, Gaël},
	month = jun,
	year = {2016},
	note = {ISSN: 1938-7228},
	pages = {642--651},
}

@inproceedings{li_preconditioned_2016,
	address = {Phoenix, Arizona},
	series = {{AAAI}'16},
	title = {Preconditioned {Stochastic} {Gradient} {Langevin} {Dynamics} for deep neural networks},
	abstract = {Effective training of deep neural networks suffers from two main issues. The first is that the parameter spaces of these models exhibit pathological curvature. Recent methods address this problem by using adaptive preconditioning for Stochastic Gradient Descent (SGD). These methods improve convergence by adapting to the local geometry of parameter space. A second issue is overfitting, which is typically addressed by early stopping. However, recent work has demonstrated that Bayesian model averaging mitigates this problem. The posterior can be sampled by using Stochastic Gradient Langevin Dynamics (SGLD). However, the rapidly changing curvature renders default SGLD methods inefficient. Here, we propose combining adaptive preconditioners with SGLD. In support of this idea, we give theoretical properties on asymptotic convergence and predictive risk. We also provide empirical results for Logistic Regression, Feedforward Neural Nets, and Convolutional Neural Nets, demonstrating that our preconditioned SGLD method gives state-of-the-art performance on these models.},
	urldate = {2021-02-11},
	booktitle = {Proceedings of the {Thirtieth} {AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Li, Chunyuan and Chen, Changyou and Carlson, David and Carin, Lawrence},
	month = feb,
	year = {2016},
	pages = {1788--1794},
}

@misc{noauthor_preconditioned_nodate,
	title = {Preconditioned {Stochastic} {Gradient} {Langevin} {Dynamics} for deep neural networks {\textbar} {Proceedings} of the {Thirtieth} {AAAI} {Conference} on {Artificial} {Intelligence}},
	url = {https://dl.acm.org/doi/abs/10.5555/3016100.3016149},
	urldate = {2021-02-11},
}

@article{duchi_adaptive_2011,
	title = {Adaptive {Subgradient} {Methods} for {Online} {Learning} and {Stochastic} {Optimization}},
	volume = {12},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v12/duchi11a.html},
	number = {61},
	urldate = {2021-02-11},
	journal = {Journal of Machine Learning Research},
	author = {Duchi, John and Hazan, Elad and Singer, Yoram},
	year = {2011},
	pages = {2121--2159},
}

@article{ward_adagrad_2019,
	title = {{AdaGrad} stepsizes: {Sharp} convergence over nonconvex landscapes, from any initialization},
	shorttitle = {{AdaGrad} stepsizes},
	url = {http://arxiv.org/abs/1806.01811},
	abstract = {Adaptive gradient methods such as AdaGrad and its variants update the stepsize in stochastic gradient descent on the fly according to the gradients received along the way; such methods have gained widespread use in large-scale optimization for their ability to converge robustly, without the need to fine-tune the stepsize schedule. Yet, the theoretical guarantees to date for AdaGrad are for online and convex optimization. We bridge this gap by providing theoretical guarantees for the convergence of AdaGrad for smooth, nonconvex functions. We show that the norm version of AdaGrad (AdaGrad-Norm) converges to a stationary point at the \${\textbackslash}mathcal\{O\}({\textbackslash}log(N)/{\textbackslash}sqrt\{N\})\$ rate in the stochastic setting, and at the optimal \${\textbackslash}mathcal\{O\}(1/N)\$ rate in the batch (non-stochastic) setting -- in this sense, our convergence guarantees are 'sharp'. In particular, the convergence of AdaGrad-Norm is robust to the choice of all hyper-parameters of the algorithm, in contrast to stochastic gradient descent whose convergence depends crucially on tuning the step-size to the (generally unknown) Lipschitz smoothness constant and level of stochastic noise on the gradient. Extensive numerical experiments are provided to corroborate our theory; moreover, the experiments suggest that the robustness of AdaGrad-Norm extends to state-of-the-art models in deep learning, without sacrificing generalization.},
	urldate = {2021-02-11},
	journal = {arXiv:1806.01811 [cs, stat]},
	author = {Ward, Rachel and Wu, Xiaoxia and Bottou, Leon},
	month = apr,
	year = {2019},
	note = {arXiv: 1806.01811},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@book{polyak_introduction_2020,
	title = {Introduction to {Optimization}},
	abstract = {This is the revised version of the book, originally published in 1987. All corrections are made with proofreading marks on the margins. I am indebted to numerous readers of the monograph who indicated typos and inaccuracies in the original text. The contribution of my friend Olvi Mangasarian and his students was extraordinary helpful. My colleague Andrey Tremba incorporated all revisions in the text; I highly appreciate his assistance. November 2010.},
	author = {Polyak, Boris},
	month = jul,
	year = {2020},
}

@article{vempala_rapid_2019-2,
	title = {Rapid {Convergence} of the {Unadjusted} {Langevin} {Algorithm}: {Isoperimetry} {Suffices}},
	volume = {32},
	shorttitle = {Rapid {Convergence} of the {Unadjusted} {Langevin} {Algorithm}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/65a99bb7a3115fdede20da98b08a370f-Abstract.html},
	language = {en},
	urldate = {2021-02-10},
	journal = {Advances in Neural Information Processing Systems},
	author = {Vempala, Santosh and Wibisono, Andre},
	year = {2019},
	pages = {8094--8106},
}

@article{luan_langevin_2020,
	title = {Langevin monte carlo rendering with gradient-based adaptation},
	volume = {39},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/3386569.3392382},
	doi = {10.1145/3386569.3392382},
	abstract = {We introduce a suite of Langevin Monte Carlo algorithms for efficient photorealistic rendering of scenes with complex light transport effects, such as caustics, interreflections, and occlusions. Our algorithms operate in primary sample space, and use the Metropolis-adjusted Langevin algorithm (MALA) to generate new samples. Drawing inspiration from state-of-the-art stochastic gradient descent procedures, we combine MALA with adaptive preconditioning and momentum schemes that re-use previously-computed first-order gradients, either in an online or in a cache-driven fashion. This combination allows MALA to adapt to the local geometry of the primary sample space, without the computational overhead associated with previous Hessian-based adaptation algorithms. We use the theory of controlled Markov chain Monte Carlo to ensure that these combinations remain ergodic, and are therefore suitable for unbiased Monte Carlo rendering. Through extensive experiments, we show that our algorithms, MALA with online and cache-driven adaptation, can successfully handle complex light transport in a large variety of scenes, leading to improved performance (on average more than 3× variance reduction at equal time, and 7× for motion blur) compared to state-of-the-art Markov chain Monte Carlo rendering algorithms.},
	number = {4},
	urldate = {2021-02-10},
	journal = {ACM Transactions on Graphics},
	author = {Luan, Fujun and Zhao, Shuang and Bala, Kavita and Gkioulekas, Ioannis},
	month = jul,
	year = {2020},
	keywords = {global illumination, langevin Monte Carlo, photorealistic rendering},
	pages = {140:140:1--140:140:16},
}

@article{chewi_exponential_2020,
	title = {Exponential ergodicity of mirror-{Langevin} diffusions},
	url = {http://arxiv.org/abs/2005.09669},
	abstract = {Motivated by the problem of sampling from ill-conditioned log-concave distributions, we give a clean non-asymptotic convergence analysis of mirror-Langevin diffusions as introduced in Zhang et al. (2020). As a special case of this framework, we propose a class of diffusions called Newton-Langevin diffusions and prove that they converge to stationarity exponentially fast with a rate which not only is dimension-free, but also has no dependence on the target distribution. We give an application of this result to the problem of sampling from the uniform distribution on a convex body using a strategy inspired by interior-point methods. Our general approach follows the recent trend of linking sampling and optimization and highlights the role of the chi-squared divergence. In particular, it yields new results on the convergence of the vanilla Langevin diffusion in Wasserstein distance.},
	urldate = {2021-02-10},
	journal = {arXiv:2005.09669 [cs, math, stat]},
	author = {Chewi, Sinho and Gouic, Thibaut Le and Lu, Chen and Maunu, Tyler and Rigollet, Philippe and Stromme, Austin J.},
	month = jun,
	year = {2020},
	note = {arXiv: 2005.09669},
	keywords = {60J25, Computer Science - Machine Learning, Mathematics - Statistics Theory},
}

@article{mou_improved_2019,
	title = {Improved {Bounds} for {Discretization} of {Langevin} {Diffusions}: {Near}-{Optimal} {Rates} without {Convexity}},
	shorttitle = {Improved {Bounds} for {Discretization} of {Langevin} {Diffusions}},
	url = {http://arxiv.org/abs/1907.11331},
	abstract = {We present an improved analysis of the Euler-Maruyama discretization of the Langevin diffusion. Our analysis does not require global contractivity, and yields polynomial dependence on the time horizon. Compared to existing approaches, we make an additional smoothness assumption, and improve the existing rate from \$O({\textbackslash}eta)\$ to \$O({\textbackslash}eta{\textasciicircum}2)\$ in terms of the KL divergence. This result matches the correct order for numerical SDEs, without suffering from exponential time dependence. When applied to algorithms for sampling and learning, this result simultaneously improves all those methods based on Dalayan's approach.},
	urldate = {2021-02-09},
	journal = {arXiv:1907.11331 [math, stat]},
	author = {Mou, Wenlong and Flammarion, Nicolas and Wainwright, Martin J. and Bartlett, Peter L.},
	month = nov,
	year = {2019},
	note = {arXiv: 1907.11331},
	keywords = {Mathematics - Probability, Mathematics - Statistics Theory, Statistics - Computation, Statistics - Machine Learning},
}

@misc{mou_improved_2019-1,
	title = {Improved {Bounds} for {Discretization} of {Langevin} {Diffusions}: {Near}-{Optimal} {Rates} without {Convexity}},
	shorttitle = {Improved {Bounds} for {Discretization} of {Langevin} {Diffusions}},
	url = {http://infoscience.epfl.ch/record/272852},
	abstract = {We consider minimizing a nonconvex, smooth function \$f\$ on a Riemannian manifold \${\textbackslash}mathcal\{M\}\$. We show that a perturbed version of Riemannian gradient descent algorithm converges to a second-order stationary point (and hence is able to escape saddle points on the manifold). The rate of convergence depends as \$1/{\textbackslash}epsilon{\textasciicircum}2\$ on the accuracy \${\textbackslash}epsilon\$, which matches a rate known only for unconstrained smooth minimization. The convergence rate depends polylogarithmically on the manifold dimension \$d\$, hence is almost dimension-free. The rate also has a polynomial dependence on the parameters describing the curvature of the manifold and the smoothness of the function. While the unconstrained problem (Euclidean setting) is well-studied, our result is the first to prove such a rate for nonconvex, manifold-constrained problems.},
	language = {en},
	urldate = {2021-02-09},
	journal = {arXiv},
	author = {Mou, Wenlong and Flammarion, Nicolas and Wainwright, Martin J. and Bartlett, Peter L.},
	year = {2019},
	note = {Number: ARTICLE},
}

@article{vempala_rapid_2019-3,
	title = {Rapid {Convergence} of the {Unadjusted} {Langevin} {Algorithm}: {Isoperimetry} {Suffices}},
	shorttitle = {Rapid {Convergence} of the {Unadjusted} {Langevin} {Algorithm}},
	url = {http://arxiv.org/abs/1903.08568},
	abstract = {We study the Unadjusted Langevin Algorithm (ULA) for sampling from a probability distribution \${\textbackslash}nu = e{\textasciicircum}\{-f\}\$ on \${\textbackslash}mathbb\{R\}{\textasciicircum}n\$. We prove a convergence guarantee in Kullback-Leibler (KL) divergence assuming \${\textbackslash}nu\$ satisfies a log-Sobolev inequality and the Hessian of \$f\$ is bounded. Notably, we do not assume convexity or bounds on higher derivatives. We also prove convergence guarantees in R{\textbackslash}'enyi divergence of order \$q {\textgreater} 1\$ assuming the limit of ULA satisfies either the log-Sobolev or Poincar{\textbackslash}'e inequality.},
	urldate = {2021-02-09},
	journal = {arXiv:1903.08568 [cs, math, stat]},
	author = {Vempala, Santosh S. and Wibisono, Andre},
	month = aug,
	year = {2019},
	note = {arXiv: 1903.08568},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning, Mathematics - Probability, Statistics - Machine Learning},
}

@misc{noauthor_adaptive_nodate,
	title = {An adaptive approach to {Langevin} {MCMC} {\textbar} {Statistics} and {Computing}},
	url = {https://dl.acm.org/doi/abs/10.1007/s11222-011-9276-6},
	urldate = {2021-02-08},
}

@inproceedings{li_convergence_2019,
	title = {On the {Convergence} of {Stochastic} {Gradient} {Descent} with {Adaptive} {Stepsizes}},
	url = {http://proceedings.mlr.press/v89/li19c.html},
	abstract = {Stochastic gradient descent is the method of choice for large scale optimization of machine learning objective functions. Yet, its performance is greatly variable and heavily depends on the choice ...},
	language = {en},
	urldate = {2021-02-08},
	booktitle = {The 22nd {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Li, Xiaoyu and Orabona, Francesco},
	month = apr,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {983--992},
}

@article{cutkosky_online_2017,
	title = {Online {Learning} {Without} {Prior} {Information}},
	url = {http://arxiv.org/abs/1703.02629},
	abstract = {The vast majority of optimization and online learning algorithms today require some prior information about the data (often in the form of bounds on gradients or on the optimal parameter value). When this information is not available, these algorithms require laborious manual tuning of various hyperparameters, motivating the search for algorithms that can adapt to the data with no prior information. We describe a frontier of new lower bounds on the performance of such algorithms, reflecting a tradeoff between a term that depends on the optimal parameter value and a term that depends on the gradients' rate of growth. Further, we construct a family of algorithms whose performance matches any desired point on this frontier, which no previous algorithm reaches.},
	urldate = {2021-02-08},
	journal = {arXiv:1703.02629 [cs, stat]},
	author = {Cutkosky, Ashok and Boahen, Kwabena},
	month = jun,
	year = {2017},
	note = {arXiv: 1703.02629},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{agarwal_efficient_2020,
	title = {Efficient {Full}-{Matrix} {Adaptive} {Regularization}},
	url = {http://arxiv.org/abs/1806.02958},
	abstract = {Adaptive regularization methods pre-multiply a descent direction by a preconditioning matrix. Due to the large number of parameters of machine learning problems, full-matrix preconditioning methods are prohibitively expensive. We show how to modify full-matrix adaptive regularization in order to make it practical and effective. We also provide a novel theoretical analysis for adaptive regularization in non-convex optimization settings. The core of our algorithm, termed GGT, consists of the efficient computation of the inverse square root of a low-rank matrix. Our preliminary experiments show improved iteration-wise convergence rates across synthetic tasks and standard deep learning benchmarks, and that the more carefully-preconditioned steps sometimes lead to a better solution.},
	urldate = {2020-12-22},
	journal = {arXiv:1806.02958 [cs, math, stat]},
	author = {Agarwal, Naman and Bullins, Brian and Chen, Xinyi and Hazan, Elad and Singh, Karan and Zhang, Cyril and Zhang, Yi},
	month = nov,
	year = {2020},
	note = {arXiv: 1806.02958},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{staib_escaping_2020,
	title = {Escaping {Saddle} {Points} with {Adaptive} {Gradient} {Methods}},
	url = {http://arxiv.org/abs/1901.09149},
	abstract = {Adaptive methods such as Adam and RMSProp are widely used in deep learning but are not well understood. In this paper, we seek a crisp, clean and precise characterization of their behavior in nonconvex settings. To this end, we first provide a novel view of adaptive methods as preconditioned SGD, where the preconditioner is estimated in an online manner. By studying the preconditioner on its own, we elucidate its purpose: it rescales the stochastic gradient noise to be isotropic near stationary points, which helps escape saddle points. Furthermore, we show that adaptive methods can efficiently estimate the aforementioned preconditioner. By gluing together these two components, we provide the first (to our knowledge) second-order convergence result for any adaptive method. The key insight from our analysis is that, compared to SGD, adaptive methods escape saddle points faster, and can converge faster overall to second-order stationary points.},
	urldate = {2020-12-22},
	journal = {arXiv:1901.09149 [cs, math, stat]},
	author = {Staib, Matthew and Reddi, Sashank J. and Kale, Satyen and Kumar, Sanjiv and Sra, Suvrit},
	month = feb,
	year = {2020},
	note = {arXiv: 1901.09149},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{malitsky_adaptive_2020,
	title = {Adaptive {Gradient} {Descent} without {Descent}},
	url = {http://arxiv.org/abs/1910.09529},
	abstract = {We present a strikingly simple proof that two rules are sufficient to automate gradient descent: 1) don't increase the stepsize too fast and 2) don't overstep the local curvature. No need for functional values, no line search, no information about the function except for the gradients. By following these rules, you get a method adaptive to the local geometry, with convergence guarantees depending only on the smoothness in a neighborhood of a solution. Given that the problem is convex, our method converges even if the global smoothness constant is infinity. As an illustration, it can minimize arbitrary continuously twice-differentiable convex function. We examine its performance on a range of convex and nonconvex problems, including logistic regression and matrix factorization.},
	urldate = {2020-12-22},
	journal = {arXiv:1910.09529 [cs, math, stat]},
	author = {Malitsky, Yura and Mishchenko, Konstantin},
	month = aug,
	year = {2020},
	note = {arXiv: 1910.09529},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{hazan_revisiting_2019,
	title = {Revisiting the {Polyak} step size},
	url = {http://arxiv.org/abs/1905.00313},
	abstract = {This paper revisits the Polyak step size schedule for convex optimization problems, proving that a simple variant of it simultaneously attains near optimal convergence rates for the gradient descent algorithm, for all ranges of strong convexity, smoothness, and Lipschitz parameters, without a-priory knowledge of these parameters.},
	urldate = {2020-12-22},
	journal = {arXiv:1905.00313 [math]},
	author = {Hazan, Elad and Kakade, Sham},
	month = may,
	year = {2019},
	note = {arXiv: 1905.00313},
	keywords = {Mathematics - Optimization and Control},
}

@article{barre_complexity_2020,
	title = {Complexity {Guarantees} for {Polyak} {Steps} with {Momentum}},
	url = {http://arxiv.org/abs/2002.00915},
	abstract = {In smooth strongly convex optimization, knowledge of the strong convexity parameter is critical for obtaining simple methods with accelerated rates. In this work, we study a class of methods, based on Polyak steps, where this knowledge is substituted by that of the optimal value, \$f\_*\$. We first show slightly improved convergence bounds than previously known for the classical case of simple gradient descent with Polyak steps, we then derive an accelerated gradient method with Polyak steps and momentum, along with convergence guarantees.},
	urldate = {2020-12-22},
	journal = {arXiv:2002.00915 [cs, math, stat]},
	author = {Barré, Mathieu and Taylor, Adrien and d'Aspremont, Alexandre},
	month = jul,
	year = {2020},
	note = {arXiv: 2002.00915},
	keywords = {Mathematics - Numerical Analysis, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{li_convergence_2019-1,
	title = {On the {Convergence} of {Stochastic} {Gradient} {Descent} with {Adaptive} {Stepsizes}},
	url = {http://arxiv.org/abs/1805.08114},
	abstract = {Stochastic gradient descent is the method of choice for large scale optimization of machine learning objective functions. Yet, its performance is greatly variable and heavily depends on the choice of the stepsizes. This has motivated a large body of research on adaptive stepsizes. However, there is currently a gap in our theoretical understanding of these methods, especially in the non-convex setting. In this paper, we start closing this gap: we theoretically analyze in the convex and non-convex settings a generalized version of the AdaGrad stepsizes. We show sufficient conditions for these stepsizes to achieve almost sure asymptotic convergence of the gradients to zero, proving the first guarantee for generalized AdaGrad stepsizes in the non-convex setting. Moreover, we show that these stepsizes allow to automatically adapt to the level of noise of the stochastic gradients in both the convex and non-convex settings, interpolating between \$O(1/T)\$ and \$O(1/{\textbackslash}sqrt\{T\})\$, up to logarithmic terms.},
	urldate = {2020-12-22},
	journal = {arXiv:1805.08114 [cs, math, stat]},
	author = {Li, Xiaoyu and Orabona, Francesco},
	month = feb,
	year = {2019},
	note = {arXiv: 1805.08114},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{mcmahan_survey_2017,
	title = {A survey of {Algorithms} and {Analysis} for {Adaptive} {Online} {Learning}},
	volume = {18},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v18/14-428.html},
	number = {90},
	urldate = {2020-11-10},
	journal = {Journal of Machine Learning Research},
	author = {McMahan, H. Brendan},
	year = {2017},
	pages = {1--50},
}

@article{mcmahan_survey_nodate,
	title = {A {Survey} of {Algorithms} and {Analysis} for {Adaptive} {Online} {Learning}},
	abstract = {We present tools for the analysis of Follow-The-Regularized-Leader (FTRL), Dual Averaging, and Mirror Descent algorithms when the regularizer (equivalently, proxfunction or learning rate schedule) is chosen adaptively based on the data. Adaptivity can be used to prove regret bounds that hold on every round, and also allows for data-dependent regret bounds as in AdaGrad-style algorithms (e.g., Online Gradient Descent with adaptive per-coordinate learning rates). We present results from a large number of prior works in a uniﬁed manner, using a modular and tight analysis that isolates the key arguments in easily re-usable lemmas. This approach strengthens previously known FTRL analysis techniques to produce bounds as tight as those achieved by potential functions or primal-dual analysis. Further, we prove a general and exact equivalence between adaptive Mirror Descent algorithms and a corresponding FTRL update, which allows us to analyze Mirror Descent algorithms in the same framework. The key to bridging the gap between Dual Averaging and Mirror Descent algorithms lies in an analysis of the FTRL-Proximal algorithm family. Our regret bounds are proved in the most general form, holding for arbitrary norms and non-smooth regularizers with time-varying weight.},
	language = {en},
	author = {McMahan, H Brendan},
	pages = {50},
}

@incollection{pavliotis_introduction_2014,
	address = {New York, NY},
	series = {Texts in {Applied} {Mathematics}},
	title = {Introduction to {Stochastic} {Differential} {Equations}},
	isbn = {978-1-4939-1323-7},
	url = {https://doi.org/10.1007/978-1-4939-1323-7_3},
	abstract = {In this chapter, we study diffusion processes at the level of paths. In particular, we study stochastic differential equations (SDEs) driven by Gaussian white noise, defined formally as the derivative of Brownian motion. In Sect. 3.1, we introduce SDEs. In Sect. 3.2, we introduce the Itô and Stratonovich stochastic integrals. In Sect. 3.3, we present the concept of a solution to an SDE. The generator, Itô’s formula, and the connection with the Fokker–Planck equation are covered in Sect. 3.4. Examples of SDEs are presented in Sect. 3.5. The Lamperti transformation and Girsanov’s theorem are discussed briefly in Sect. 3.6. Linear SDEs are studied in Sect. 3.7. Bibliographical remarks and exercises can be found in Sects. 3.8 and 3.9, respectively.},
	language = {en},
	urldate = {2020-10-26},
	booktitle = {Stochastic {Processes} and {Applications}: {Diffusion} {Processes}, the {Fokker}-{Planck} and {Langevin} {Equations}},
	publisher = {Springer},
	author = {Pavliotis, Grigorios A.},
	editor = {Pavliotis, Grigorios  A.},
	year = {2014},
	doi = {10.1007/978-1-4939-1323-7_3},
	keywords = {Brownian Motion, Planck Equation, Stochastic Differential Equation, Stochastic Resonance, Strong Solution},
	pages = {55--85},
}

@incollection{pavliotis_exit_2014,
	address = {New York, NY},
	series = {Texts in {Applied} {Mathematics}},
	title = {Exit {Problems} for {Diffusion} {Processes} and {Applications}},
	isbn = {978-1-4939-1323-7},
	url = {https://doi.org/10.1007/978-1-4939-1323-7_7},
	abstract = {In this chapter, we develop techniques for calculating the statistics of the time that it takes for a diffusion process in a bounded domain to reach the boundary of the domain. We then use this formalism to study the problem of Brownian motion in a bistable potential. Applications such as stochastic resonance and the modeling of Brownian motors are also presented. In Sect. 7.1, we motivate the techniques that we will develop in this chapter by looking at the problem of Brownian motion in bistable potentials. In Sect. 7.2, we obtain a boundary value problem for the mean exit time of a diffusion process from a domain. We then use this formalism in Sect. 7.3 to calculate the escape rate of a Brownian particle from a potential well. The phenomenon of stochastic resonance is investigated in Sect. 7.4. Brownian motors are studied in Sect. 7.5. Bibliographical remarks and exercises can be found in Sects. 7.6 and 7.7, respectively.},
	language = {en},
	urldate = {2020-10-26},
	booktitle = {Stochastic {Processes} and {Applications}: {Diffusion} {Processes}, the {Fokker}-{Planck} and {Langevin} {Equations}},
	publisher = {Springer},
	author = {Pavliotis, Grigorios A.},
	editor = {Pavliotis, Grigorios  A.},
	year = {2014},
	doi = {10.1007/978-1-4939-1323-7_7},
	keywords = {Brownian Particle, Exit Time, Planck Equation, Singular Perturbation Theory, Stochastic Resonance},
	pages = {235--266},
}

@incollection{pavliotis_modeling_2014,
	address = {New York, NY},
	series = {Texts in {Applied} {Mathematics}},
	title = {Modeling with {Stochastic} {Differential} {Equations}},
	isbn = {978-1-4939-1323-7},
	url = {https://doi.org/10.1007/978-1-4939-1323-7_5},
	abstract = {When the white noise in a stochastic differential equation is approximated by a smoother process, then in the limit as we remove the regularization, we obtain the Stratonovich stochastic equation. This is usually called the Wong–Zakai theorem. In this section, we derive the limiting Stratonovich SDE for a particular class of regularization of the white noise process using singular perturbation theory for Markov processes. In particular, we consider colored noise, which we model as a Gaussian stationary diffusion process, i.e., the Ornstein–Uhlenbeck process.},
	language = {en},
	urldate = {2020-10-26},
	booktitle = {Stochastic {Processes} and {Applications}: {Diffusion} {Processes}, the {Fokker}-{Planck} and {Langevin} {Equations}},
	publisher = {Springer},
	author = {Pavliotis, Grigorios A.},
	editor = {Pavliotis, Grigorios  A.},
	year = {2014},
	doi = {10.1007/978-1-4939-1323-7_5},
	keywords = {Colored Noise, Ornstein-Uhlenbeck Process, Stochastic Theta Method, Stuart-Landau Equation, Wong-Zakai Theorem},
	pages = {139--179},
}

@incollection{pavliotis_diffusion_2014,
	address = {New York, NY},
	series = {Texts in {Applied} {Mathematics}},
	title = {Diffusion {Processes}},
	isbn = {978-1-4939-1323-7},
	url = {https://doi.org/10.1007/978-1-4939-1323-7_2},
	abstract = {In this chapter, we study some of the basic properties of Markov stochastic processes, and in particular, the properties of diffusion processes. In Sect. 2.1, we present various examples of Markov processes in discrete and continuous time. In Sect. 2.2, we give the precise definition of a Markov process and we derive the fundamental equation in the theory of Markov processes, the Chapman–Kolmogorov equation. In Sect. 2.3, we introduce the concept of the generator of a Markov process. In Sect. 2.4, we study ergodic Markov processes. In Sect. 2.5, we introduce diffusion processes, and we derive the forward and backward Kolmogorov equations. Discussion and bibliographical remarks are presented in Sect. 2.6, and exercises can be found in Sect. 2.7.},
	language = {en},
	urldate = {2020-10-26},
	booktitle = {Stochastic {Processes} and {Applications}: {Diffusion} {Processes}, the {Fokker}-{Planck} and {Langevin} {Equations}},
	publisher = {Springer},
	author = {Pavliotis, Grigorios A.},
	editor = {Pavliotis, Grigorios  A.},
	year = {2014},
	doi = {10.1007/978-1-4939-1323-7_2},
	keywords = {Invariant Measure, Kolmogorov Equation, Markov Process, Transition Function, Uhlenbeck Process},
	pages = {29--54},
}

@incollection{pavliotis_fokkerplanck_2014,
	address = {New York, NY},
	series = {Texts in {Applied} {Mathematics}},
	title = {The {Fokker}–{Planck} {Equation}},
	isbn = {978-1-4939-1323-7},
	url = {https://doi.org/10.1007/978-1-4939-1323-7_4},
	abstract = {In Chap. 2, we derived the backward and forward (Fokker–Planck) Kolmogorov equations. The Fokker–Planck equation enables us to calculate the transition probability density, which we can use to calculate the expectation value of observables of a diffusion process. In this chapter, we study various properties of this equation such as existence and uniqueness of solutions, long-time asymptotics, boundary conditions, and spectral properties of the Fokker–Planck operator. We also study in some detail various examples of diffusion processes and of the associated Fokker–Planck equation. We will restrict attention to time-homogeneous diffusion processes, for which the drift and diffusion coefficients do not depend on time.},
	language = {en},
	urldate = {2020-10-26},
	booktitle = {Stochastic {Processes} and {Applications}: {Diffusion} {Processes}, the {Fokker}-{Planck} and {Langevin} {Equations}},
	publisher = {Springer},
	author = {Pavliotis, Grigorios A.},
	editor = {Pavliotis, Grigorios  A.},
	year = {2014},
	doi = {10.1007/978-1-4939-1323-7_4},
	keywords = {Logarithmic Sobolev Inequalities (LSI), Ornstein-Uhlenbeck Process, Smoluchowski Dynamics, Stationary Fokker-Planck Equation, Transition Probability Density},
	pages = {87--137},
}

@incollection{pavliotis_langevin_2014,
	address = {New York, NY},
	series = {Texts in {Applied} {Mathematics}},
	title = {The {Langevin} {Equation}},
	isbn = {978-1-4939-1323-7},
	url = {https://doi.org/10.1007/978-1-4939-1323-7_6},
	abstract = {In this chapter, we study the Langevin equation and the associated Fokker –Planck equation. In Sect. 6.1, we introduce the equation and study some of the main properties of the corresponding Fokker–Planck equation. In Sect. 6.2 we give an elementary introduction to the theories of hypoellipticity and hypocoercivity. In Sect. 6.3, we calculate the spectrum of the generator and Fokker–Planck operators for the Langevin equation in a harmonic potential. In Sect. 6.4, we study Hermite polynomial expansions of solutions to the Fokker–Planck equation. In Sect. 6.5, we study the overdamped and underdamped limits for the Langevin equation. In Sect. 6.6, we study the problem of Brownian motion in a periodic potential. Bibliographical remarks and exercises can be found in Sects. 6.7 and 6.8, respectively.},
	language = {en},
	urldate = {2020-10-26},
	booktitle = {Stochastic {Processes} and {Applications}: {Diffusion} {Processes}, the {Fokker}-{Planck} and {Langevin} {Equations}},
	publisher = {Springer},
	author = {Pavliotis, Grigorios A.},
	editor = {Pavliotis, Grigorios  A.},
	year = {2014},
	doi = {10.1007/978-1-4939-1323-7_6},
	keywords = {Fokker-Planck Operator, Hypocoercivity, Langevin Dynamics, Periodic Potential, Underdamped Limit},
	pages = {181--233},
}

@incollection{pavliotis_introduction_2014-1,
	address = {New York, NY},
	series = {Texts in {Applied} {Mathematics}},
	title = {Introduction to {Stochastic} {Processes}},
	isbn = {978-1-4939-1323-7},
	url = {https://doi.org/10.1007/978-1-4939-1323-7_1},
	abstract = {In this chapter, we present some basic results from the theory of stochastic processes and investigate the properties of some standard continuous-time stochastic processes. In Sect. 1.1, we give the definition of a stochastic process. In Sect. 1.2, we present some properties of stationary stochastic processes. In Sect. 1.3, we introduce Brownian motion and study some of its properties. Various examples of stochastic processes in continuous time are presented in Sect. 1.4. The Karhunen–Loève expansion, one of the most useful tools for representing stochastic processes and random fields, is presented in Sect. 1.5. Further discussion and bibliographical comments are presented in Sect. 1.6. Section 1.7 contains exercises.},
	language = {en},
	urldate = {2020-10-26},
	booktitle = {Stochastic {Processes} and {Applications}: {Diffusion} {Processes}, the {Fokker}-{Planck} and {Langevin} {Equations}},
	publisher = {Springer},
	author = {Pavliotis, Grigorios A.},
	editor = {Pavliotis, Grigorios  A.},
	year = {2014},
	doi = {10.1007/978-1-4939-1323-7_1},
	keywords = {Autocorrelation Function, Brownian Motion, Covariance Function, Gaussian Process, Stochastic Process},
	pages = {1--27},
}

@incollection{pavliotis_derivation_2014,
	address = {New York, NY},
	series = {Texts in {Applied} {Mathematics}},
	title = {Derivation of the {Langevin} {Equation}},
	isbn = {978-1-4939-1323-7},
	url = {https://doi.org/10.1007/978-1-4939-1323-7_8},
	abstract = {In this chapter, we derive the Langevin equation from a simple mechanical model for a small system (which we will refer to as a Brownian particle) that is in contact with a thermal reservoir that is at thermodynamic equilibrium at time t = 0. The full dynamics, Brownian particle plus thermal reservoir, are assumed to be Hamiltonian. The derivation proceeds in three steps. First, we derive a closed stochastic integrodifferential equation for the dynamics of the Brownian particle, the generalized Langevin equation (GLE). In the second step, we approximate the GLE by a finite-dimensional Markovian equation in an extended phase space. Finally, we use singular perturbation theory for Markov processes to derive the Langevin equation, under the assumption of rapidly decorrelating noise. This derivation provides a partial justification for the use of stochastic differential equations, in particular the Langevin equation, in the modeling of physical systems.},
	language = {en},
	urldate = {2020-10-26},
	booktitle = {Stochastic {Processes} and {Applications}: {Diffusion} {Processes}, the {Fokker}-{Planck} and {Langevin} {Equations}},
	publisher = {Springer},
	author = {Pavliotis, Grigorios A.},
	editor = {Pavliotis, Grigorios  A.},
	year = {2014},
	doi = {10.1007/978-1-4939-1323-7_8},
	keywords = {Autocorrelation Function, Brownian Particle, Gibbs Measure, Heat Bath, Langevin Equation},
	pages = {267--282},
}

@incollection{pavliotis_linear_2014,
	address = {New York, NY},
	series = {Texts in {Applied} {Mathematics}},
	title = {Linear {Response} {Theory} for {Diffusion} {Processes}},
	isbn = {978-1-4939-1323-7},
	url = {https://doi.org/10.1007/978-1-4939-1323-7_9},
	abstract = {In this chapter, we study the effect of a weak external forcing on a system at equilibrium. The forcing moves the system away from equilibrium, and we are interested in understanding the response of the system to this forcing. We study this problem for ergodic diffusion processes using perturbation theory. In particular, we develop linear response theory. The analysis of weakly perturbed systems leads to fundamental results such as the fluctuation–dissipation theorem and to the Green–Kubo formula, which enables us to calculate transport coefficients.},
	language = {en},
	urldate = {2020-10-26},
	booktitle = {Stochastic {Processes} and {Applications}: {Diffusion} {Processes}, the {Fokker}-{Planck} and {Langevin} {Equations}},
	publisher = {Springer},
	author = {Pavliotis, Grigorios A.},
	editor = {Pavliotis, Grigorios  A.},
	year = {2014},
	doi = {10.1007/978-1-4939-1323-7_9},
	keywords = {Causality Principle, Dissipation Theorem, Functional Central Limit Theorem, Kubo Formula, Linear Response Theory},
	pages = {283--296},
}

@misc{noauthor_euclidean_nodate,
	title = {\{{Euclidean}, metric, and {Wasserstein}\} gradient flows: an overview {\textbar} {SpringerLink}},
	url = {https://link.springer.com/article/10.1007/s13373-017-0101-1},
	urldate = {2020-10-24},
}

@article{chewi_exponential_2020-1,
	title = {Exponential ergodicity of mirror-{Langevin} diffusions},
	url = {http://arxiv.org/abs/2005.09669},
	abstract = {Motivated by the problem of sampling from ill-conditioned log-concave distributions, we give a clean non-asymptotic convergence analysis of mirror-Langevin diffusions as introduced in Zhang et al. (2020). As a special case of this framework, we propose a class of diffusions called Newton-Langevin diffusions and prove that they converge to stationarity exponentially fast with a rate which not only is dimension-free, but also has no dependence on the target distribution. We give an application of this result to the problem of sampling from the uniform distribution on a convex body using a strategy inspired by interior-point methods. Our general approach follows the recent trend of linking sampling and optimization and highlights the role of the chi-squared divergence. In particular, it yields new results on the convergence of the vanilla Langevin diffusion in Wasserstein distance.},
	urldate = {2020-10-24},
	journal = {arXiv:2005.09669 [cs, math, stat]},
	author = {Chewi, Sinho and Gouic, Thibaut Le and Lu, Chen and Maunu, Tyler and Rigollet, Philippe and Stromme, Austin J.},
	month = jun,
	year = {2020},
	note = {arXiv: 2005.09669},
	keywords = {60J25, Computer Science - Machine Learning, Mathematics - Statistics Theory},
}

@article{azizan_stochastic_2019,
	title = {Stochastic {Gradient}/{Mirror} {Descent}: {Minimax} {Optimality} and {Implicit} {Regularization}},
	shorttitle = {Stochastic {Gradient}/{Mirror} {Descent}},
	url = {http://arxiv.org/abs/1806.00952},
	abstract = {Stochastic descent methods (of the gradient and mirror varieties) have become increasingly popular in optimization. In fact, it is now widely recognized that the success of deep learning is not only due to the special deep architecture of the models, but also due to the behavior of the stochastic descent methods used, which play a key role in reaching "good" solutions that generalize well to unseen data. In an attempt to shed some light on why this is the case, we revisit some minimax properties of stochastic gradient descent (SGD) for the square loss of linear models---originally developed in the 1990's---and extend them to general stochastic mirror descent (SMD) algorithms for general loss functions and nonlinear models. In particular, we show that there is a fundamental identity which holds for SMD (and SGD) under very general conditions, and which implies the minimax optimality of SMD (and SGD) for sufficiently small step size, and for a general class of loss functions and general nonlinear models. We further show that this identity can be used to naturally establish other properties of SMD (and SGD), namely convergence and implicit regularization for over-parameterized linear models (in what is now being called the "interpolating regime"), some of which have been shown in certain cases in prior literature. We also argue how this identity can be used in the so-called "highly over-parameterized" nonlinear setting (where the number of parameters far exceeds the number of data points) to provide insights into why SMD (and SGD) may have similar convergence and implicit regularization properties for deep learning.},
	urldate = {2020-10-23},
	journal = {arXiv:1806.00952 [cs, math, stat]},
	author = {Azizan, Navid and Hassibi, Babak},
	month = jan,
	year = {2019},
	note = {arXiv: 1806.00952},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{nacson_stochastic_2019,
	title = {Stochastic {Gradient} {Descent} on {Separable} {Data}: {Exact} {Convergence} with a {Fixed} {Learning} {Rate}},
	shorttitle = {Stochastic {Gradient} {Descent} on {Separable} {Data}},
	url = {http://arxiv.org/abs/1806.01796},
	abstract = {Stochastic Gradient Descent (SGD) is a central tool in machine learning. We prove that SGD converges to zero loss, even with a fixed (non-vanishing) learning rate - in the special case of homogeneous linear classifiers with smooth monotone loss functions, optimized on linearly separable data. Previous works assumed either a vanishing learning rate, iterate averaging, or loss assumptions that do not hold for monotone loss functions used for classification, such as the logistic loss. We prove our result on a fixed dataset, both for sampling with or without replacement. Furthermore, for logistic loss (and similar exponentially-tailed losses), we prove that with SGD the weight vector converges in direction to the \$L\_2\$ max margin vector as \$O(1/{\textbackslash}log(t))\$ for almost all separable datasets, and the loss converges as \$O(1/t)\$ - similarly to gradient descent. Lastly, we examine the case of a fixed learning rate proportional to the minibatch size. We prove that in this case, the asymptotic convergence rate of SGD (with replacement) does not depend on the minibatch size in terms of epochs, if the support vectors span the data. These results may suggest an explanation to similar behaviors observed in deep networks, when trained with SGD.},
	urldate = {2020-10-22},
	journal = {arXiv:1806.01796 [cs, stat]},
	author = {Nacson, Mor Shpigel and Srebro, Nathan and Soudry, Daniel},
	month = mar,
	year = {2019},
	note = {arXiv: 1806.01796},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@incollection{gunasekar_implicit_2017,
	title = {Implicit {Regularization} in {Matrix} {Factorization}},
	url = {http://papers.nips.cc/paper/7195-implicit-regularization-in-matrix-factorization.pdf},
	urldate = {2020-10-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	publisher = {Curran Associates, Inc.},
	author = {Gunasekar, Suriya and Woodworth, Blake E and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nati},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
	pages = {6151--6159},
}

@article{livingstone_information-geometric_2014,
	title = {Information-{Geometric} {Markov} {Chain} {Monte} {Carlo} {Methods} {Using} {Diffusions}},
	volume = {16},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1099-4300/16/6/3074},
	doi = {10.3390/e16063074},
	abstract = {Recent work incorporating geometric ideas in Markov chain Monte Carlo is reviewed in order to highlight these advances and their possible application in a range of domains beyond statistics. A full exposition of Markov chains and their use in Monte Carlo simulation for statistical inference and molecular dynamics is provided, with particular emphasis on methods based on Langevin diffusions. After this, geometric concepts in Markov chain Monte Carlo are introduced. A full derivation of the Langevin diffusion on a Riemannian manifold is given, together with a discussion of the appropriate Riemannian metric choice for different problems. A survey of applications is provided, and some open questions are discussed.},
	language = {en},
	number = {6},
	urldate = {2020-10-17},
	journal = {Entropy},
	author = {Livingstone, Samuel and Girolami, Mark},
	month = jun,
	year = {2014},
	note = {Number: 6
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Bayesian inference, Markov chain Monte Carlo, computational statistics, diffusions, information geometry, machine learning, statistical mechanics},
	pages = {3074--3102},
}

@article{rossky_brownian_1978,
	title = {Brownian dynamics as smart {Monte} {Carlo} simulation},
	volume = {69},
	issn = {0021-9606},
	url = {https://aip.scitation.org/doi/10.1063/1.436415},
	doi = {10.1063/1.436415},
	number = {10},
	urldate = {2020-10-17},
	journal = {The Journal of Chemical Physics},
	author = {Rossky, P. J. and Doll, J. D. and Friedman, H. L.},
	month = nov,
	year = {1978},
	note = {Publisher: American Institute of Physics},
	pages = {4628--4633},
}

@article{stramer_langevin-type_1999,
	title = {Langevin-{Type} {Models} {I}: {Diffusions} with {Given} {Stationary} {Distributions} and their {Discretizations}*},
	volume = {1},
	issn = {1573-7713},
	shorttitle = {Langevin-{Type} {Models} {I}},
	url = {https://doi.org/10.1023/A:1010086427957},
	doi = {10.1023/A:1010086427957},
	abstract = {We describe algorithms for estimating a given measure π known up to a constant of proportionality, based on a large class of diffusions (extending the Langevin model) for which π is invariant. We show that under weak conditions one can choose from this class in such a way that the diffusions converge at exponential rate to π, and one can even ensure that convergence is independent of the starting point of the algorithm. When convergence is less than exponential we show that it is often polynomial at verifiable rates. We then consider methods of discretizing the diffusion in time, and find methods which inherit the convergence rates of the continuous time process. These contrast with the behavior of the naive or Euler discretization, which can behave badly even in simple cases. Our results are described in detail in one dimension only, although extensions to higher dimensions are also briefly described.},
	language = {en},
	number = {3},
	urldate = {2020-10-17},
	journal = {Methodology And Computing In Applied Probability},
	author = {Stramer, O. and Tweedie, R. L.},
	month = oct,
	year = {1999},
	pages = {283--306},
}

@article{roberts_langevin_2002,
	title = {Langevin {Diffusions} and {Metropolis}-{Hastings} {Algorithms}},
	volume = {4},
	issn = {1573-7713},
	url = {https://doi.org/10.1023/A:1023562417138},
	doi = {10.1023/A:1023562417138},
	abstract = {We consider a class of Langevin diffusions with state-dependent volatility. The volatility of the diffusion is chosen so as to make the stationary distribution of the diffusion with respect to its natural clock, a heated version of the stationary density of interest. The motivation behind this construction is the desire to construct uniformly ergodic diffusions with required stationary densities. Discrete time algorithms constructed by Hastings accept reject mechanisms are constructed from discretisations of the algorithms, and the properties of these algorithms are investigated.},
	language = {en},
	number = {4},
	urldate = {2020-10-17},
	journal = {Methodology And Computing In Applied Probability},
	author = {Roberts, G. O. and Stramer, O.},
	month = dec,
	year = {2002},
	pages = {337--357},
}

@article{roberts_exponential_1996-1,
	title = {Exponential convergence of {Langevin} distributions and their discrete approximations},
	volume = {2},
	issn = {1350-7265},
	url = {https://projecteuclid.org/euclid.bj/1178291835},
	abstract = {In this paper we consider a continuous-time method of approximating a given distribution ππ{\textless}math overflow="scroll"{\textgreater} {\textless}mi{\textgreater}π{\textless}/mi{\textgreater} {\textless}/math{\textgreater} using the Langevin diffusion dLt=dWt+12∇logπ(Lt)dtdLt=dWt+12∇logπ(Lt)dt{\textless}math overflow="scroll"{\textgreater} {\textless}mo{\textgreater}d{\textless}/mo{\textgreater} {\textless}msub{\textgreater}{\textless}mstyle fontweight="bold"{\textgreater}{\textless}mrow{\textgreater}{\textless}mi{\textgreater}L{\textless}/mi{\textgreater} {\textless}/mrow{\textgreater}{\textless}/mstyle{\textgreater} {\textless}mi{\textgreater}t{\textless}/mi{\textgreater}{\textless}/msub{\textgreater}{\textless}mo{\textgreater}={\textless}/mo{\textgreater}{\textless}mo{\textgreater}d{\textless}/mo{\textgreater}{\textless}msub{\textgreater}{\textless}mstyle fontweight="bold"{\textgreater}{\textless}mrow{\textgreater}{\textless}mi{\textgreater}W{\textless}/mi{\textgreater} {\textless}/mrow{\textgreater}{\textless}/mstyle{\textgreater} {\textless}mi{\textgreater}t{\textless}/mi{\textgreater}{\textless}/msub{\textgreater}{\textless}mo{\textgreater}+{\textless}/mo{\textgreater}{\textless}mfrac{\textgreater}{\textless}mrow{\textgreater}{\textless}mn{\textgreater}1{\textless}/mn{\textgreater} {\textless}/mrow{\textgreater}{\textless}mrow{\textgreater}{\textless}mn{\textgreater}2{\textless}/mn{\textgreater} {\textless}/mrow{\textgreater}{\textless}/mfrac{\textgreater}{\textless}mo{\textgreater}∇{\textless}/mo{\textgreater}{\textless}mo{\textgreater}log{\textless}/mo{\textgreater}{\textless}mi{\textgreater}π{\textless}/mi{\textgreater}{\textless}mo{\textgreater}({\textless}/mo{\textgreater}{\textless}msub{\textgreater}{\textless}mstyle fontweight="bold"{\textgreater}{\textless}mrow{\textgreater}{\textless}mi{\textgreater}L{\textless}/mi{\textgreater} {\textless}/mrow{\textgreater}{\textless}/mstyle{\textgreater} {\textless}mi{\textgreater}t{\textless}/mi{\textgreater}{\textless}/msub{\textgreater}{\textless}mo{\textgreater}){\textless}/mo{\textgreater}{\textless}mo{\textgreater}d{\textless}/mo{\textgreater}{\textless}mi{\textgreater}t{\textless}/mi{\textgreater} {\textless}/math{\textgreater}. We find conditions under which this diffusion converges exponentially quickly to ππ{\textless}math overflow="scroll"{\textgreater} {\textless}mi{\textgreater}π{\textless}/mi{\textgreater} {\textless}/math{\textgreater} or does not: in one dimension, these are essentially that for distributions with exponential tails of the form π(x)∝exp(−γ∣∣x∣∣β)π(x)∝exp(-γ{\textbar}x{\textbar}β){\textless}math overflow="scroll"{\textgreater} {\textless}mi{\textgreater}π{\textless}/mi{\textgreater} {\textless}mo{\textgreater}({\textless}/mo{\textgreater}{\textless}mi{\textgreater}x{\textless}/mi{\textgreater}{\textless}mo{\textgreater}){\textless}/mo{\textgreater}{\textless}mo{\textgreater}∝{\textless}/mo{\textgreater}{\textless}mo{\textgreater}exp{\textless}/mo{\textgreater}{\textless}mo{\textgreater}({\textless}/mo{\textgreater}{\textless}mo{\textgreater}-{\textless}/mo{\textgreater}{\textless}mi{\textgreater}γ{\textless}/mi{\textgreater}{\textless}mo{\textgreater}{\textbar}{\textless}/mo{\textgreater}{\textless}mi{\textgreater}x{\textless}/mi{\textgreater}{\textless}msup{\textgreater}{\textless}mo{\textgreater}{\textbar}{\textless}/mo{\textgreater} {\textless}mrow{\textgreater}{\textless}mi{\textgreater}β{\textless}/mi{\textgreater} {\textless}/mrow{\textgreater}{\textless}/msup{\textgreater}{\textless}mo{\textgreater}){\textless}/mo{\textgreater} {\textless}/math{\textgreater}, 0{\textless}β{\textless}∞0{\textless}β{\textless}∞{\textless}math overflow="scroll"{\textgreater} {\textless}mn{\textgreater}0{\textless}/mn{\textgreater} {\textless}mi{\textgreater}\&lt;{\textless}/mi{\textgreater}{\textless}mi{\textgreater}β{\textless}/mi{\textgreater}{\textless}mi{\textgreater}\&lt;{\textless}/mi{\textgreater}{\textless}mn{\textgreater}∞{\textless}/mn{\textgreater} {\textless}/math{\textgreater}, exponential convergence occurs if and only if β≥1β≥1{\textless}math overflow="scroll"{\textgreater} {\textless}mi{\textgreater}β{\textless}/mi{\textgreater} {\textless}mo{\textgreater}≥{\textless}/mo{\textgreater}{\textless}mn{\textgreater}1{\textless}/mn{\textgreater} {\textless}/math{\textgreater}. We then consider conditions under which the discrete approximations to the diffusion converge. We first show that even when the diffusion itself converges, naive discretizations need not do so. We then consider a 'Metropolis-adjusted' version of the algorithm, and find conditions under which this also converges at an exponential rate: perhaps surprisingly, even the Metropolized version need not converge exponentially fast even if the diffusion does. We briefly discuss a truncated form of the algorithm which, in practice, should avoid the difficulties of the other forms.},
	language = {EN},
	number = {4},
	urldate = {2020-10-16},
	journal = {Bernoulli},
	author = {Roberts, Gareth O. and Tweedie, Richard L.},
	month = dec,
	year = {1996},
	mrnumber = {MR1440273},
	zmnumber = {0870.60027},
	note = {Publisher: Bernoulli Society for Mathematical Statistics and Probability},
	keywords = {Hastings algorithms, Langevin models, Markov chain Monte Carlo, Metropolis algorithms, diffusions, discrete approximations, geometric ergodicity, irreducible Markov processes, posterior distributions},
	pages = {341--363},
}

@article{saumard_log-concavity_2014,
	title = {Log-concavity and strong log-concavity: {A} review},
	volume = {8},
	issn = {1935-7516},
	shorttitle = {Log-concavity and strong log-concavity},
	url = {https://projecteuclid.org/euclid.ssu/1418134163},
	doi = {10.1214/14-SS107},
	abstract = {We review and formulate results concerning log-concavity and strong-log-concavity in both discrete and continuous settings. We show how preservation of log-concavity and strong log-concavity on ℝR{\textbackslash}mathbb\{R\} under convolution follows from a fundamental monotonicity result of Efron (1965). We provide a new proof of Efron’s theorem using the recent asymmetric Brascamp-Lieb inequality due to Otto and Menz (2013). Along the way we review connections between log-concavity and other areas of mathematics and statistics, including concentration of measure, log-Sobolev inequalities, convex geometry, MCMC algorithms, Laplace approximations, and machine learning.},
	language = {EN},
	urldate = {2020-10-16},
	journal = {Statistics Surveys},
	author = {Saumard, Adrien and Wellner, Jon A.},
	year = {2014},
	mrnumber = {MR3290441},
	zmnumber = {1360.62055},
	note = {Publisher: The author, under a Creative Commons Attribution License},
	keywords = {Concave, convex, convolution, inequalities, log-concave, monotone, preservation, strong log-concave},
	pages = {45--114},
}

@article{nakkiran_deep_2019,
	title = {Deep {Double} {Descent}: {Where} {Bigger} {Models} and {More} {Data} {Hurt}},
	shorttitle = {Deep {Double} {Descent}},
	url = {http://arxiv.org/abs/1912.02292},
	abstract = {We show that a variety of modern deep learning tasks exhibit a "double-descent" phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.},
	urldate = {2020-10-16},
	journal = {arXiv:1912.02292 [cs, stat]},
	author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.02292},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{razin_implicit_2020,
	title = {Implicit {Regularization} in {Deep} {Learning} {May} {Not} {Be} {Explainable} by {Norms}},
	url = {http://arxiv.org/abs/2005.06398},
	abstract = {Mathematically characterizing the implicit regularization induced by gradient-based optimization is a longstanding pursuit in the theory of deep learning. A widespread hope is that a characterization based on minimization of norms may apply, and a standard test-bed for studying this prospect is matrix factorization (matrix completion via linear neural networks). It is an open question whether norms can explain the implicit regularization in matrix factorization. The current paper resolves this open question in the negative, by proving that there exist natural matrix factorization problems on which the implicit regularization drives all norms (and quasi-norms) towards infinity. Our results suggest that, rather than perceiving the implicit regularization via norms, a potentially more useful interpretation is minimization of rank. We demonstrate empirically that this interpretation extends to a certain class of non-linear neural networks, and hypothesize that it may be key to explaining generalization in deep learning.},
	urldate = {2020-10-16},
	journal = {arXiv:2005.06398 [cs, stat]},
	author = {Razin, Noam and Cohen, Nadav},
	month = may,
	year = {2020},
	note = {arXiv: 2005.06398},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{gunasekar_characterizing_2020,
	title = {Characterizing {Implicit} {Bias} in {Terms} of {Optimization} {Geometry}},
	url = {http://arxiv.org/abs/1802.08246},
	abstract = {We study the implicit bias of generic optimization methods, such as mirror descent, natural gradient descent, and steepest descent with respect to different potentials and norms, when optimizing underdetermined linear regression or separable linear classification problems. We explore the question of whether the specific global minimum (among the many possible global minima) reached by an algorithm can be characterized in terms of the potential or norm of the optimization geometry, and independently of hyperparameter choices such as step-size and momentum.},
	urldate = {2020-10-16},
	journal = {arXiv:1802.08246 [cs, stat]},
	author = {Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
	month = jun,
	year = {2020},
	note = {arXiv: 1802.08246},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{soudry_implicit_2018,
	title = {The {Implicit} {Bias} of {Gradient} {Descent} on {Separable} {Data}},
	url = {http://arxiv.org/abs/1710.10345},
	abstract = {We examine gradient descent on unregularized logistic regression problems, with homogeneous linear predictors on linearly separable datasets. We show the predictor converges to the direction of the max-margin (hard margin SVM) solution. The result also generalizes to other monotone decreasing loss functions with an infimum at infinity, to multi-class problems, and to training a weight layer in a deep network in a certain restricted setting. Furthermore, we show this convergence is very slow, and only logarithmic in the convergence of the loss itself. This can help explain the benefit of continuing to optimize the logistic or cross-entropy loss even after the training error is zero and the training loss is extremely small, and, as we show, even if the validation loss increases. Our methodology can also aid in understanding implicit regularization n more complex models and with other optimization methods.},
	urldate = {2020-10-16},
	journal = {arXiv:1710.10345 [cs, stat]},
	author = {Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
	month = dec,
	year = {2018},
	note = {arXiv: 1710.10345},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{chatterji_oracle_2020,
	title = {Oracle lower bounds for stochastic gradient sampling algorithms},
	url = {http://arxiv.org/abs/2002.00291},
	abstract = {We consider the problem of sampling from a strongly log-concave density in \${\textbackslash}mathbb\{R\}{\textasciicircum}\{d\}\$, and prove an information theoretic lower bound on the number of stochastic gradient queries of the log density needed. Several popular sampling algorithms (including many Markov chain Monte Carlo methods) operate by using stochastic gradients of the log density to generate a sample; our results establish an information theoretic limit for all these algorithms. We show that for every algorithm, there exists a well-conditioned strongly log-concave target density for which the distribution of points generated by the algorithm would be at least \${\textbackslash}varepsilon\$ away from the target in total variation distance if the number of gradient queries is less than \${\textbackslash}Omega({\textbackslash}sigma{\textasciicircum}2 d/{\textbackslash}varepsilon{\textasciicircum}2)\$, where \${\textbackslash}sigma{\textasciicircum}2 d\$ is the variance of the stochastic gradient. Our lower bound follows by combining the ideas of Le Cam deficiency routinely used in the comparison of statistical experiments along with standard information theoretic tools used in lower bounding Bayes risk functions. To the best of our knowledge our results provide the first nontrivial dimension-dependent lower bound for this problem.},
	urldate = {2020-10-12},
	journal = {arXiv:2002.00291 [cs, math, stat]},
	author = {Chatterji, Niladri S. and Bartlett, Peter L. and Long, Philip M.},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.00291},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@article{moitra_fast_2020,
	title = {Fast {Convergence} for {Langevin} {Diffusion} with {Matrix} {Manifold} {Structure}},
	url = {http://arxiv.org/abs/2002.05576},
	abstract = {In this paper, we study the problem of sampling from distributions of the form p(x) {\textbackslash}propto e{\textasciicircum}\{-{\textbackslash}beta f(x)\} for some function f whose values and gradients we can query. This mode of access to f is natural in the scenarios in which such problems arise, for instance sampling from posteriors in parametric Bayesian models. Classical results show that a natural random walk, Langevin diffusion, mixes rapidly when f is convex. Unfortunately, even in simple examples, the applications listed above will entail working with functions f that are nonconvex -- for which sampling from p may in general require an exponential number of queries. In this paper, we study one aspect of nonconvexity relevant for modern machine learning applications: existence of invariances (symmetries) in the function f, as a result of which the distribution p will have manifolds of points with equal probability. We give a recipe for proving mixing time bounds of Langevin dynamics in order to sample from manifolds of local optima of the function f in settings where the distribution is well-concentrated around them. We specialize our arguments to classic matrix factorization-like Bayesian inference problems where we get noisy measurements A(XX{\textasciicircum}T), X {\textbackslash}in R{\textasciicircum}\{d {\textbackslash}times k\} of a low-rank matrix, i.e. f(X) = {\textbackslash}{\textbar}A(XX{\textasciicircum}T) - b{\textbackslash}{\textbar}{\textasciicircum}2\_2, X {\textbackslash}in R{\textasciicircum}\{d {\textbackslash}times k\}, and {\textbackslash}beta the inverse of the variance of the noise. Such functions f are invariant under orthogonal transformations, and include problems like matrix factorization, sensing, completion. Beyond sampling, Langevin dynamics is a popular toy model for studying stochastic gradient descent. Along these lines, we believe that our work is an important first step towards understanding how SGD behaves when there is a high degree of symmetry in the space of parameters the produce the same output.},
	urldate = {2020-09-17},
	journal = {arXiv:2002.05576 [cs, math, stat]},
	author = {Moitra, Ankur and Risteski, Andrej},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.05576},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning, Mathematics - Probability, Statistics - Machine Learning},
}

@article{miyato_spectral_2018,
	title = {Spectral {Normalization} for {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1802.05957},
	abstract = {One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques.},
	urldate = {2020-09-17},
	journal = {arXiv:1802.05957 [cs, stat]},
	author = {Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.05957},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{hsieh_finding_2018,
	title = {Finding {Mixed} {Nash} {Equilibria} of {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1811.02002},
	abstract = {We reconsider the training objective of Generative Adversarial Networks (GANs) from the mixed Nash Equilibria (NE) perspective. Inspired by the classical prox methods, we develop a novel algorithmic framework for GANs via an infinite-dimensional two-player game and prove rigorous convergence rates to the mixed NE, resolving the longstanding problem that no provably convergent algorithm exists for general GANs. We then propose a principled procedure to reduce our novel prox methods to simple sampling routines, leading to practically efficient algorithms. Finally, we provide experimental evidence that our approach outperforms methods that seek pure strategy equilibria, such as SGD, Adam, and RMSProp, both in speed and quality.},
	urldate = {2020-09-17},
	journal = {arXiv:1811.02002 [cs, stat]},
	author = {Hsieh, Ya-Ping and Liu, Chen and Cevher, Volkan},
	month = oct,
	year = {2018},
	note = {arXiv: 1811.02002},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@incollection{hsieh_mirrored_2018,
	title = {Mirrored {Langevin} {Dynamics}},
	url = {http://papers.nips.cc/paper/7552-mirrored-langevin-dynamics.pdf},
	urldate = {2020-09-17},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Hsieh, Ya-Ping and Kavis, Ali and Rolland, Paul and Cevher, Volkan},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {2878--2887},
}

@misc{noauthor_mail_nodate-1,
	title = {Mail - leello.dadi@epfl.ch},
	url = {https://ewa.epfl.ch/owa/#path=/mail/inbox},
	urldate = {2020-09-16},
}

@article{jain_accelerating_2017,
	title = {Accelerating {Stochastic} {Gradient} {Descent} {For} {Least} {Squares} {Regression}},
	url = {http://arxiv.org/abs/1704.08227},
	abstract = {There is widespread sentiment that it is not possible to effectively utilize fast gradient methods (e.g. Nesterov's acceleration, conjugate gradient, heavy ball) for the purposes of stochastic optimization due to their instability and error accumulation, a notion made precise in d'Aspremont 2008 and Devolder, Glineur, and Nesterov 2014. This work considers these issues for the special case of stochastic approximation for the least squares regression problem, and our main result refutes the conventional wisdom by showing that acceleration can be made robust to statistical errors. In particular, this work introduces an accelerated stochastic gradient method that provably achieves the minimax optimal statistical risk faster than stochastic gradient descent. Critical to the analysis is a sharp characterization of accelerated stochastic gradient descent as a stochastic process. We hope this characterization gives insights towards the broader question of designing simple and effective accelerated stochastic methods for more general convex and non-convex optimization problems.},
	urldate = {2020-09-07},
	journal = {arXiv:1704.08227 [cs, math, stat]},
	author = {Jain, Prateek and Kakade, Sham M. and Kidambi, Rahul and Netrapalli, Praneeth and Sidford, Aaron},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.08227
version: 1},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@article{dauber_can_2020,
	title = {Can {Implicit} {Bias} {Explain} {Generalization}? {Stochastic} {Convex} {Optimization} as a {Case} {Study}},
	shorttitle = {Can {Implicit} {Bias} {Explain} {Generalization}?},
	url = {http://arxiv.org/abs/2003.06152},
	abstract = {The notion of implicit bias, or implicit regularization, has been suggested as a means to explain the surprising generalization ability of modern-days overparameterized learning algorithms. This notion refers to the tendency of the optimization algorithm towards a certain structured solution that often generalizes well. Recently, several papers have studied implicit regularization and were able to identify this phenomenon in various scenarios. We revisit this paradigm in arguably the simplest non-trivial setup, and study the implicit bias of Stochastic Gradient Descent (SGD) in the context of Stochastic Convex Optimization. As a first step, we provide a simple construction that rules out the existence of a {\textbackslash}emph\{distribution-independent\} implicit regularizer that governs the generalization ability of SGD. We then demonstrate a learning problem that rules out a very general class of {\textbackslash}emph\{distribution-dependent\} implicit regularizers from explaining generalization, which includes strongly convex regularizers as well as non-degenerate norm-based regularizations. Certain aspects of our constructions point out to significant difficulties in providing a comprehensive explanation of an algorithm's generalization performance by solely arguing about its implicit regularization properties.},
	urldate = {2020-09-01},
	journal = {arXiv:2003.06152 [cs, stat]},
	author = {Dauber, Assaf and Feder, Meir and Koren, Tomer and Livni, Roi},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.06152},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{agarwal_information-theoretic_2011,
	title = {Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization},
	url = {http://arxiv.org/abs/1009.0571},
	abstract = {Relative to the large literature on upper bounds on complexity of convex optimization, lesser attention has been paid to the fundamental hardness of these problems. Given the extensive use of convex optimization in machine learning and statistics, gaining an understanding of these complexity-theoretic issues is important. In this paper, we study the complexity of stochastic convex optimization in an oracle model of computation. We improve upon known results and obtain tight minimax complexity estimates for various function classes.},
	urldate = {2020-08-17},
	journal = {arXiv:1009.0571 [cs, math, stat]},
	author = {Agarwal, Alekh and Bartlett, Peter L. and Ravikumar, Pradeep and Wainwright, Martin J.},
	month = nov,
	year = {2011},
	note = {arXiv: 1009.0571},
	keywords = {Electrical Engineering and Systems Science - Systems and Control, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{vempala_rapid_2019-4,
	title = {Rapid {Convergence} of the {Unadjusted} {Langevin} {Algorithm}: {Isoperimetry} {Suffices}},
	shorttitle = {Rapid {Convergence} of the {Unadjusted} {Langevin} {Algorithm}},
	url = {http://arxiv.org/abs/1903.08568},
	abstract = {We study the Unadjusted Langevin Algorithm (ULA) for sampling from a probability distribution \${\textbackslash}nu = e{\textasciicircum}\{-f\}\$ on \${\textbackslash}mathbb\{R\}{\textasciicircum}n\$. We prove a convergence guarantee in Kullback-Leibler (KL) divergence assuming \${\textbackslash}nu\$ satisfies a log-Sobolev inequality and the Hessian of \$f\$ is bounded. Notably, we do not assume convexity or bounds on higher derivatives. We also prove convergence guarantees in R{\textbackslash}'enyi divergence of order \$q {\textgreater} 1\$ assuming the limit of ULA satisfies either the log-Sobolev or Poincar{\textbackslash}'e inequality.},
	urldate = {2020-08-04},
	journal = {arXiv:1903.08568 [cs, math, stat]},
	author = {Vempala, Santosh S. and Wibisono, Andre},
	month = aug,
	year = {2019},
	note = {arXiv: 1903.08568},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning, Mathematics - Probability, Statistics - Machine Learning},
}

@misc{noauthor_mail_nodate-2,
	title = {Mail - leello.dadi@epfl.ch},
	url = {https://ewa.epfl.ch/owa/#path=/mail/search},
	urldate = {2020-07-31},
}

@article{wibisono_sampling_2018,
	title = {Sampling as optimization in the space of measures: {The} {Langevin} dynamics as a composite optimization problem},
	shorttitle = {Sampling as optimization in the space of measures},
	url = {http://arxiv.org/abs/1802.08089},
	abstract = {We study sampling as optimization in the space of measures. We focus on gradient flow-based optimization with the Langevin dynamics as a case study. We investigate the source of the bias of the unadjusted Langevin algorithm (ULA) in discrete time, and consider how to remove or reduce the bias. We point out the difficulty is that the heat flow is exactly solvable, but neither its forward nor backward method is implementable in general, except for Gaussian data. We propose the symmetrized Langevin algorithm (SLA), which should have a smaller bias than ULA, at the price of implementing a proximal gradient step in space. We show SLA is in fact consistent for Gaussian target measure, whereas ULA is not. We also illustrate various algorithms explicitly for Gaussian target measure, including gradient descent, proximal gradient, and Forward-Backward, and show they are all consistent.},
	urldate = {2020-07-31},
	journal = {arXiv:1802.08089 [cs, math, stat]},
	author = {Wibisono, Andre},
	month = jun,
	year = {2018},
	note = {arXiv: 1802.08089},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{ghorbani_when_2020,
	title = {When {Do} {Neural} {Networks} {Outperform} {Kernel} {Methods}?},
	url = {http://arxiv.org/abs/2006.13409},
	abstract = {For a certain scaling of the initialization of stochastic gradient descent (SGD), wide neural networks (NN) have been shown to be well approximated by reproducing kernel Hilbert space (RKHS) methods. Recent empirical work showed that, for some classification tasks, RKHS methods can replace NNs without a large loss in performance. On the other hand, two-layers NNs are known to encode richer smoothness classes than RKHS and we know of special examples for which SGD-trained NN provably outperform RKHS. This is true even in the wide network limit, for a different scaling of the initialization. How can we reconcile the above claims? For which tasks do NNs outperform RKHS? If feature vectors are nearly isotropic, RKHS methods suffer from the curse of dimensionality, while NNs can overcome it by learning the best low-dimensional representation. Here we show that this curse of dimensionality becomes milder if the feature vectors display the same low-dimensional structure as the target function, and we precisely characterize this tradeoff. Building on these results, we present a model that can capture in a unified framework both behaviors observed in earlier work. We hypothesize that such a latent low-dimensional structure is present in image classification. We test numerically this hypothesis by showing that specific perturbations of the training distribution degrade the performances of RKHS methods much more significantly than NNs.},
	urldate = {2020-06-29},
	journal = {arXiv:2006.13409 [cs, math, stat]},
	author = {Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.13409},
	keywords = {62J99 (Primary), Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@misc{noauthor_concentration_nodate,
	title = {Concentration - {Probability} {Hammers} for {Stochastic} {Nails}},
	url = {https://app.gitbook.com/@leello-tadesse/s/probability-hammers-for-stochastic-nails/concentration},
	urldate = {2020-06-22},
}

@article{woodworth_open_nodate,
	title = {Open {Problem}: {The} {Oracle} {Complexity} of {Convex} {Optimization} with {Limited} {Memory}},
	abstract = {We note that known methods achieving the optimal oracle complexity for ﬁrst order convex optimization require quadratic memory, and ask whether this is necessary, and more broadly seek to characterize the minimax number of ﬁrst order queries required to optimize a convex Lipschitz function subject to a memory constraint.},
	language = {en},
	author = {Woodworth, Blake and Srebro, Nathan},
	pages = {9},
}

@article{dauber_can_2020-1,
	title = {Can {Implicit} {Bias} {Explain} {Generalization}? {Stochastic} {Convex} {Optimization} as a {Case} {Study}},
	shorttitle = {Can {Implicit} {Bias} {Explain} {Generalization}?},
	url = {http://arxiv.org/abs/2003.06152},
	abstract = {The notion of implicit bias, or implicit regularization, has been suggested as a means to explain the surprising generalization ability of modern-days overparameterized learning algorithms. This notion refers to the tendency of the optimization algorithm towards a certain structured solution that often generalizes well. Recently, several papers have studied implicit regularization and were able to identify this phenomenon in various scenarios. We revisit this paradigm in arguably the simplest non-trivial setup, and study the implicit bias of Stochastic Gradient Descent (SGD) in the context of Stochastic Convex Optimization. As a first step, we provide a simple construction that rules out the existence of a {\textbackslash}emph\{distribution-independent\} implicit regularizer that governs the generalization ability of SGD. We then demonstrate a learning problem that rules out a very general class of {\textbackslash}emph\{distribution-dependent\} implicit regularizers from explaining generalization, which includes strongly convex regularizers as well as non-degenerate norm-based regularizations. Certain aspects of our constructions point out to significant difficulties in providing a comprehensive explanation of an algorithm's generalization performance by solely arguing about its implicit regularization properties.},
	urldate = {2020-06-10},
	journal = {arXiv:2003.06152 [cs, stat]},
	author = {Dauber, Assaf and Feder, Meir and Koren, Tomer and Livni, Roi},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.06152},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{bach_consistency_2007,
	title = {Consistency of trace norm minimization},
	url = {http://arxiv.org/abs/0710.2848},
	abstract = {Regularization by the sum of singular values, also referred to as the trace norm, is a popular technique for estimating low rank rectangular matrices. In this paper, we extend some of the consistency results of the Lasso to provide necessary and sufficient conditions for rank consistency of trace norm minimization with the square loss. We also provide an adaptive version that is rank consistent even when the necessary condition for the non adaptive version is not fulfilled.},
	urldate = {2020-06-01},
	journal = {arXiv:0710.2848 [cs]},
	author = {Bach, Francis},
	month = oct,
	year = {2007},
	note = {arXiv: 0710.2848},
	keywords = {Computer Science - Machine Learning},
}

@article{kakade_regularization_nodate,
	title = {Regularization {Techniques} for {Learning} with {Matrices}},
	abstract = {There is growing body of learning problems for which it is natural to organize the parameters into a matrix. As a result, it becomes easy to impose sophisticated prior knowledge by appropriately regularizing the parameters under some matrix norm. This work describes and analyzes a systematic method for constructing such matrix-based regularization techniques. In particular, we focus on how the underlying statistical properties of a given problem can help us decide which regularization function is appropriate.},
	language = {en},
	author = {Kakade, Sham M and Shalev-Shwartz, Shai and Tewari, Ambuj},
	pages = {26},
}

@article{wimalawarne_theoretical_2016,
	title = {Theoretical and {Experimental} {Analyses} of {Tensor}-{Based} {Regression} and {Classification}},
	volume = {28},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/NECO_a_00815},
	doi = {10.1162/NECO_a_00815},
	abstract = {We theoretically and experimentally investigate tensor-based regression and classification. Our focus is regularization with various tensor norms, including the overlapped trace norm, the latent trace norm, and the scaled latent trace norm. We first give dual optimization methods using the alternating direction method of multipliers, which is computationally efficient when the number of training samples is moderate. We then theoretically derive an excess risk bound for each tensor norm and clarify their behavior. Finally, we perform extensive experiments using simulated and real data and demonstrate the superiority of tensor-based learning methods over vector- and matrix-based learning methods.},
	number = {4},
	urldate = {2020-06-01},
	journal = {Neural Computation},
	author = {Wimalawarne, Kishan and Tomioka, Ryota and Sugiyama, Masashi},
	month = feb,
	year = {2016},
	note = {Publisher: MIT Press},
	pages = {686--715},
}

@article{luo_support_nodate,
	title = {Support {Matrix} {Machines}},
	abstract = {In many classiﬁcation problems such as electroencephalogram (EEG) classiﬁcation and image classiﬁcation, the input features are naturally represented as matrices rather than vectors or scalars. In general, the structure information of the original feature matrix is useful and informative for data analysis tasks such as classiﬁcation. One typical structure information is the correlation between columns or rows in the feature matrix. To leverage this kind of structure information, we propose a new classiﬁcation method that we call support matrix machine (SMM). Specifically, SMM is deﬁned as a hinge loss plus a so-called spectral elastic net penalty which is a spectral extension of the conventional elastic net over a matrix. The spectral elastic net enjoys a property of grouping effect, i.e., strongly correlated columns or rows tend to be selected altogether or not. Since the optimization problem for SMM is convex, this encourages us to devise an alternating direction method of multipliers (ADMM) algorithm for solving the problem. Experimental results on EEG and image classiﬁcation data show that our model is more robust and efﬁcient than the state-of-the-art methods.},
	language = {en},
	author = {Luo, Luo and Xie, Yubo and Zhang, Zhihua and Li, Wu-Jun},
	pages = {10},
}

@misc{noauthor_63_nodate,
	title = {6.3. {Preprocessing} data — scikit-learn 0.23.1 documentation},
	url = {https://scikit-learn.org/stable/modules/preprocessing.html},
	urldate = {2020-06-01},
}

@article{pong_trace_2010,
	title = {Trace {Norm} {Regularization}: {Reformulations}, {Algorithms}, and {Multi}-{Task} {Learning}},
	volume = {20},
	issn = {1052-6234, 1095-7189},
	shorttitle = {Trace {Norm} {Regularization}},
	url = {http://epubs.siam.org/doi/10.1137/090763184},
	doi = {10.1137/090763184},
	abstract = {We consider a recently proposed optimization formulation of multi-task learning based on trace norm regularized least squares. While this problem may be formulated as a semideﬁnite program (SDP), its size is beyond general SDP solvers. Previous solution approaches apply proximal gradient methods to solve the primal problem. We derive new primal and dual reformulations of this problem, including a reduced dual formulation that involves minimizing a convex quadratic function over an operator-norm ball in matrix space. This reduced dual problem may be solved by gradient-projection methods, with each projection involving a singular value decomposition. The dual approach is compared with existing approaches and its practical eﬀectiveness is illustrated on simulations and an application to gene expression pattern analysis.},
	language = {en},
	number = {6},
	urldate = {2020-05-15},
	journal = {SIAM Journal on Optimization},
	author = {Pong, Ting Kei and Tseng, Paul and Ji, Shuiwang and Ye, Jieping},
	month = jan,
	year = {2010},
	pages = {3465--3489},
}

@article{luo_support_nodate,
	title = {Support {Matrix} {Machines}},
	abstract = {In many classiﬁcation problems such as electroencephalogram (EEG) classiﬁcation and image classiﬁcation, the input features are naturally represented as matrices rather than vectors or scalars. In general, the structure information of the original feature matrix is useful and informative for data analysis tasks such as classiﬁcation. One typical structure information is the correlation between columns or rows in the feature matrix. To leverage this kind of structure information, we propose a new classiﬁcation method that we call support matrix machine (SMM). Specifically, SMM is deﬁned as a hinge loss plus a so-called spectral elastic net penalty which is a spectral extension of the conventional elastic net over a matrix. The spectral elastic net enjoys a property of grouping effect, i.e., strongly correlated columns or rows tend to be selected altogether or not. Since the optimization problem for SMM is convex, this encourages us to devise an alternating direction method of multipliers (ADMM) algorithm for solving the problem. Experimental results on EEG and image classiﬁcation data show that our model is more robust and efﬁcient than the state-of-the-art methods.},
	language = {en},
	author = {Luo, Luo and Xie, Yubo and Zhang, Zhihua and Li, Wu-Jun},
	pages = {10},
}

@article{zheng_sparse_2018,
	title = {Sparse {Support} {Matrix} {Machine}},
	volume = {76},
	issn = {0031-3203},
	url = {http://www.sciencedirect.com/science/article/pii/S0031320317304016},
	doi = {10.1016/j.patcog.2017.10.003},
	abstract = {Modern technologies have been producing data with complex intrinsic structures, which can be naturally represented as two-dimensional matrices, such as gray digital images, and electroencephalography (EEG) signals. When processing these data for classification, traditional classifiers, such as support vector machine (SVM) and logistic regression, have to reshape each input matrix into a feature vector, resulting in the loss of structural information. In contrast, modern classification methods such as support matrix machine capture these structures by regularizing the regression matrix to be low-rank. These methods assume that all entities within each input matrix can serve as the explanatory features for its label. However, in real-world applications, many features are redundant and useless for certain classification tasks, thus it is important to perform feature selection to filter out redundant features for more interpretable modeling. In this paper, we tackle this issue, and propose a novel classification technique called Sparse Support Matrix Machine (SSMM), which is favored for taking both the intrinsic structure of each input matrix and feature selection into consideration simultaneously. The proposed SSMM is defined as a hinge loss for model fitting, with a new regularization on the regression matrix. Specifically, the new regularization term is a linear combination of nuclear norm and ℓ1 norm, to consider the low-rank property and sparse property respectively. The resulting optimization problem is convex, and motivates us to propose a novel and efficient generalized forward-backward algorithm for solving it. To evaluate the effectiveness of our method, we conduct comparative studies on the applications of both image and EEG data classification problems. Our approach achieves state-of-the-art performance consistently. It shows the promise of our SSMM method on real-world applications.},
	language = {en},
	urldate = {2020-06-01},
	journal = {Pattern Recognition},
	author = {Zheng, Qingqing and Zhu, Fengyuan and Qin, Jing and Chen, Badong and Heng, Pheng-Ann},
	month = apr,
	year = {2018},
	keywords = {Classification, Low rank, Matrix analysis, Sparse, Support vector machine},
	pages = {715--726},
}

@article{wimalawarne_theoretical_2016,
	title = {Theoretical and {Experimental} {Analyses} of {Tensor}-{Based} {Regression} and {Classification}},
	volume = {28},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/NECO_a_00815},
	doi = {10.1162/NECO_a_00815},
	abstract = {We theoretically and experimentally investigate tensor-based regression and classification. Our focus is regularization with various tensor norms, including the overlapped trace norm, the latent trace norm, and the scaled latent trace norm. We first give dual optimization methods using the alternating direction method of multipliers, which is computationally efficient when the number of training samples is moderate. We then theoretically derive an excess risk bound for each tensor norm and clarify their behavior. Finally, we perform extensive experiments using simulated and real data and demonstrate the superiority of tensor-based learning methods over vector- and matrix-based learning methods.},
	number = {4},
	urldate = {2020-06-01},
	journal = {Neural Computation},
	author = {Wimalawarne, Kishan and Tomioka, Ryota and Sugiyama, Masashi},
	month = feb,
	year = {2016},
	note = {Publisher: MIT Press},
	pages = {686--715},
}

@inproceedings{jaggi_simple_2010,
	title = {A {Simple} {Algorithm} for {Nuclear} {Norm} {Regularized} {Problems}},
	url = {https://openreview.net/forum?id=BkWpxsW_ZH},
	abstract = {Optimization problems with a nuclear norm regularization, such as e.g. low norm matrix factorizations, have seen many applications recently. We propose a new approximation algorithm building upon...},
	urldate = {2020-06-01},
	author = {Jaggi, Martin and Sulovsk\&\#xFD, Marek},
	month = jan,
	year = {2010},
}

@article{pong_trace_2010-1,
	title = {Trace {Norm} {Regularization}: {Reformulations}, {Algorithms}, and {Multi}-{Task} {Learning}},
	volume = {20},
	issn = {1052-6234},
	shorttitle = {Trace {Norm} {Regularization}},
	url = {https://epubs.siam.org/doi/abs/10.1137/090763184},
	doi = {10.1137/090763184},
	abstract = {We consider a recently proposed optimization formulation of multi-task learning based on trace norm regularized least squares. While this problem may be formulated as a semidefinite program (SDP), its size is beyond general SDP solvers. Previous solution approaches apply proximal gradient methods to solve the primal problem. We derive new primal and dual reformulations of this problem, including a reduced dual formulation that involves minimizing a convex quadratic function over an operator-norm ball in matrix space. This reduced dual problem may be solved by gradient-projection methods, with each projection involving a singular value decomposition. The dual approach is compared with existing approaches and its practical effectiveness is illustrated on simulations and an application to gene expression pattern analysis.},
	number = {6},
	urldate = {2020-06-01},
	journal = {SIAM Journal on Optimization},
	author = {Pong, Ting Kei and Tseng, Paul and Ji, Shuiwang and Ye, Jieping},
	month = jan,
	year = {2010},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {3465--3489},
}

@article{maurer_excess_nodate,
	title = {Excess risk bounds for multitask learning with trace norm regularization},
	abstract = {Trace norm regularization is a popular method of multitask learning. We give excess risk bounds with explicit dependence on the number of tasks, the number of examples per task and properties of the data distribution. The bounds are independent of the dimension of the input space, which may be inﬁnite as in the case of reproducing kernel Hilbert spaces. A byproduct of the proof are bounds on the expected norm of sums of random positive semideﬁnite matrices with subexponential moments.},
	language = {en},
	author = {Maurer, Andreas and Pontil, Massimiliano},
	pages = {22},
}

@inproceedings{harchaoui_large-scale_2012,
	title = {Large-scale image classification with trace-norm regularization},
	doi = {10.1109/CVPR.2012.6248078},
	abstract = {With the advent of larger image classification datasets such as ImageNet, designing scalable and efficient multi-class classification algorithms is now an important challenge. We introduce a new scalable learning algorithm for large-scale multi-class image classification, based on the multinomial logistic loss and the trace-norm regularization penalty. Reframing the challenging non-smooth optimization problem into a surrogate infinite-dimensional optimization problem with a regular ℓ1-regularization penalty, we propose a simple and provably efficient accelerated coordinate descent algorithm. Furthermore, we show how to perform efficient matrix computations in the compressed domain for quantized dense visual features, scaling up to 100,000s examples, 1,000s-dimensional features, and 100s of categories. Promising experimental results on the "Fungus", "Ungulate", and "Vehicles" subsets of ImageNet are presented, where we show that our approach performs significantly better than state-of-the-art approaches for Fisher vectors with 16 Gaussians.},
	booktitle = {2012 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Harchaoui, Zaid and Douze, Matthijs and Paulin, Mattis and Dudik, Miroslav and Malick, Jérôme},
	month = jun,
	year = {2012},
	note = {ISSN: 1063-6919},
	keywords = {Acceleration, Convergence, ImageNet, Logistics, Matrix decomposition, Optimization, Training, Vectors, accelerated coordinate descent algorithm, compressed domain, image classification, infinite-dimensional optimization problem, large-scale multiclass image classification, learning (artificial intelligence), matrix algebra, matrix computation, multinomial logistic loss, nonsmooth optimization problem, optimisation, quantized dense visual feature, regular ℓ1-regularization penalty, scalable learning algorithm, trace-norm regularization penalty},
	pages = {3386--3393},
}

@article{argyriou_when_nodate,
	title = {When {Is} {There} a {Representer} {Theorem}? {Vector} {Versus} {Matrix} {Regularizers}},
	abstract = {We consider a general class of regularization methods which learn a vector of parameters on the basis of linear measurements. It is well known that if the regularizer is a nondecreasing function of the L2 norm, then the learned vector is a linear combination of the input data. This result, known as the representer theorem, lies at the basis of kernel-based methods in machine learning. In this paper, we prove the necessity of the above condition, in the case of differentiable regularizers. We further extend our analysis to regularization methods which learn a matrix, a problem which is motivated by the application to multi-task learning. In this context, we study a more general representer theorem, which holds for a larger class of regularizers. We provide a necessary and sufﬁcient condition characterizing this class of matrix regularizers and we highlight some concrete examples of practical importance. Our analysis uses basic principles from matrix theory, especially the useful notion of matrix nondecreasing functions.},
	language = {en},
	author = {Argyriou, Andreas and Micchelli, Charles A and Pontil, Massimiliano},
	pages = {23},
}

@article{argyriou_when_nodate-1,
	title = {When {Is} {There} a {Representer} {Theorem}? {Vector} {Versus} {Matrix} {Regularizers}},
	abstract = {We consider a general class of regularization methods which learn a vector of parameters on the basis of linear measurements. It is well known that if the regularizer is a nondecreasing function of the L2 norm, then the learned vector is a linear combination of the input data. This result, known as the representer theorem, lies at the basis of kernel-based methods in machine learning. In this paper, we prove the necessity of the above condition, in the case of differentiable regularizers. We further extend our analysis to regularization methods which learn a matrix, a problem which is motivated by the application to multi-task learning. In this context, we study a more general representer theorem, which holds for a larger class of regularizers. We provide a necessary and sufﬁcient condition characterizing this class of matrix regularizers and we highlight some concrete examples of practical importance. Our analysis uses basic principles from matrix theory, especially the useful notion of matrix nondecreasing functions.},
	language = {en},
	author = {Argyriou, Andreas and Micchelli, Charles A and Pontil, Massimiliano},
	pages = {23},
}

@article{jaggi_simple_nodate,
	title = {A {Simple} {Algorithm} for {Nuclear} {Norm} {Regularized} {Problems}},
	abstract = {Optimization problems with a nuclear norm regularization, such as e.g. low norm matrix factorizations, have seen many applications recently. We propose a new approximation algorithm building upon the recent sparse approximate SDP solver of (Hazan, 2008). The experimental eﬃciency of our method is demonstrated on large matrix completion problems such as the Netﬂix dataset. The algorithm comes with strong convergence guarantees, and can be interpreted as a ﬁrst theoretically justiﬁed variant of Simon-Funk-type SVD heuristics. The method is free of tuning parameters, and very easy to parallelize.},
	language = {en},
	author = {Jaggi, Martin and Sulovský, Marek},
	pages = {8},
}

@misc{noauthor_python_nodate,
	title = {python - {Row}-wise outer product on sparse matrices},
	url = {https://stackoverflow.com/questions/57099722/row-wise-outer-product-on-sparse-matrices},
	urldate = {2020-05-26},
	journal = {Stack Overflow},
	note = {Library Catalog: stackoverflow.com},
}

@article{mitzenmacher_probability_nodate,
	title = {Probability and {Computing}},
	language = {en},
	author = {Mitzenmacher, Michael and Upfal, Eli},
	pages = {490},
}

@article{mitzenmacher_probability_nodate-1,
	title = {Probability and {Computing}},
	language = {en},
	author = {Mitzenmacher, Michael and Upfal, Eli},
	pages = {490},
}

@article{srebro_large-margin_nodate,
	title = {Large-{Margin} {Matrix} {Factorization}},
	abstract = {We present a novel approach to collaborative prediction, using low-norm instead of low-rank factorizations. The approach is inspired by, and has strong connections to, large-margin linear discrimination. We show how to learn low-norm factorizations by solving a semi-deﬁnite program, and present generalization error bounds based on analyzing the Rademacher complexity of low-norm factorizations.},
	language = {en},
	author = {Srebro, Nathan and Rennie, Jason and Jaakkola, Tommi},
	pages = {10},
}

@article{li_exponential_2019,
	title = {An {Exponential} {Learning} {Rate} {Schedule} for {Deep} {Learning}},
	url = {http://arxiv.org/abs/1910.07454},
	abstract = {Intriguing empirical evidence exists that deep learning can work well with exoticschedules for varying the learning rate. This paper suggests that the phenomenon may be due to Batch Normalization or BN, which is ubiquitous and provides benefits in optimization and generalization across all standard architectures. The following new results are shown about BN with weight decay and momentum (in other words, the typical use case which was not considered in earlier theoretical analyses of stand-alone BN. 1. Training can be done using SGD with momentum and an exponentially increasing learning rate schedule, i.e., learning rate increases by some \$(1 +{\textbackslash}alpha)\$ factor in every epoch for some \${\textbackslash}alpha {\textgreater}0\$. (Precise statement in the paper.) To the best of our knowledge this is the first time such a rate schedule has been successfully used, let alone for highly successful architectures. As expected, such training rapidly blows up network weights, but the net stays well-behaved due to normalization. 2. Mathematical explanation of the success of the above rate schedule: a rigorous proof that it is equivalent to the standard setting of BN + SGD + StandardRate Tuning + Weight Decay + Momentum. This equivalence holds for other normalization layers as well, Group Normalization, LayerNormalization, Instance Norm, etc. 3. A worked-out toy example illustrating the above linkage of hyper-parameters. Using either weight decay or BN alone reaches global minimum, but convergence fails when both are used.},
	urldate = {2020-05-15},
	journal = {arXiv:1910.07454 [cs, stat]},
	author = {Li, Zhiyuan and Arora, Sanjeev},
	month = nov,
	year = {2019},
	note = {arXiv: 1910.07454},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{arora_implicit_2019,
	title = {Implicit {Regularization} in {Deep} {Matrix} {Factorization}},
	url = {http://arxiv.org/abs/1905.13655},
	abstract = {Efforts to understand the generalization mystery in deep learning have led to the belief that gradient-based optimization induces a form of implicit regularization, a bias towards models of low "complexity." We study the implicit regularization of gradient descent over deep linear neural networks for matrix completion and sensing, a model referred to as deep matrix factorization. Our first finding, supported by theory and experiments, is that adding depth to a matrix factorization enhances an implicit tendency towards low-rank solutions, oftentimes leading to more accurate recovery. Secondly, we present theoretical and empirical arguments questioning a nascent view by which implicit regularization in matrix factorization can be captured using simple mathematical norms. Our results point to the possibility that the language of standard regularizers may not be rich enough to fully encompass the implicit regularization brought forth by gradient-based optimization.},
	urldate = {2020-05-15},
	journal = {arXiv:1905.13655 [cs, stat]},
	author = {Arora, Sanjeev and Cohen, Nadav and Hu, Wei and Luo, Yuping},
	month = oct,
	year = {2019},
	note = {arXiv: 1905.13655},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{mishra_low-rank_2013,
	title = {Low-{Rank} {Optimization} with {Trace} {Norm} {Penalty}},
	volume = {23},
	issn = {1052-6234, 1095-7189},
	url = {http://epubs.siam.org/doi/10.1137/110859646},
	doi = {10.1137/110859646},
	abstract = {The paper addresses the problem of low-rank trace norm minimization. We propose an algorithm that alternates between ﬁxed-rank optimization and rank-one updates. The ﬁxed-rank optimization is characterized by an eﬃcient factorization that makes the trace norm diﬀerentiable in the search space and the computation of duality gap numerically tractable. The search space is nonlinear but is equipped with a Riemannian structure that leads to eﬃcient computations. We present a second-order trust-region algorithm with a guaranteed quadratic rate of convergence. Overall, the proposed optimization scheme converges superlinearly to the global solution while maintaining complexity that is linear in the number of rows and columns of the matrix. To compute a set of solutions eﬃciently for a grid of regularization parameters we propose a predictor-corrector approach that outperforms the naive warm-restart approach on the ﬁxed-rank quotient manifold. The performance of the proposed algorithm is illustrated on problems of low-rank matrix completion and multivariate linear regression.},
	language = {en},
	number = {4},
	urldate = {2020-05-15},
	journal = {SIAM Journal on Optimization},
	author = {Mishra, B. and Meyer, G. and Bach, F. and Sepulchre, R.},
	month = jan,
	year = {2013},
	pages = {2124--2149},
}

@article{recht_simpler_nodate,
	title = {A {Simpler} {Approach} to {Matrix} {Completion}},
	abstract = {This paper provides the best bounds to date on the number of randomly sampled entries required to reconstruct an unknown low-rank matrix. These results improve on prior work by Cande`s and Recht (2009), Cande`s and Tao (2009), and Keshavan et al. (2009). The reconstruction is accomplished by minimizing the nuclear norm, or sum of the singular values, of the hidden matrix subject to agreement with the provided entries. If the underlying matrix satisﬁes a certain incoherence condition, then the number of entries required is equal to a quadratic logarithmic factor times the number of parameters in the singular value decomposition. The proof of this assertion is short, self contained, and uses very elementary analysis. The novel techniques herein are based on recent work in quantum information theory.},
	language = {en},
	author = {Recht, Benjamin},
	pages = {18},
}

@article{toh_accelerated_nodate,
	title = {An accelerated proximal gradient algorithm for nuclear norm regularized linear least squares problems},
	language = {en},
	author = {Toh, Kim-Chuan and Yun, Sangwoon},
	pages = {31},
}

@misc{noauthor_optimization_nodate,
	title = {Optimization {Online} - {An} accelerated proximal gradient algorithm for nuclear norm regularized least squares problems},
	url = {http://www.optimization-online.org/DB_HTML/2009/03/2268.html},
	urldate = {2020-05-15},
}

@article{pong_trace_2010-2,
	title = {Trace {Norm} {Regularization}: {Reformulations}, {Algorithms}, and {Multi}-{Task} {Learning}},
	volume = {20},
	issn = {1052-6234, 1095-7189},
	shorttitle = {Trace {Norm} {Regularization}},
	url = {http://epubs.siam.org/doi/10.1137/090763184},
	doi = {10.1137/090763184},
	abstract = {We consider a recently proposed optimization formulation of multi-task learning based on trace norm regularized least squares. While this problem may be formulated as a semideﬁnite program (SDP), its size is beyond general SDP solvers. Previous solution approaches apply proximal gradient methods to solve the primal problem. We derive new primal and dual reformulations of this problem, including a reduced dual formulation that involves minimizing a convex quadratic function over an operator-norm ball in matrix space. This reduced dual problem may be solved by gradient-projection methods, with each projection involving a singular value decomposition. The dual approach is compared with existing approaches and its practical eﬀectiveness is illustrated on simulations and an application to gene expression pattern analysis.},
	language = {en},
	number = {6},
	urldate = {2020-05-15},
	journal = {SIAM Journal on Optimization},
	author = {Pong, Ting Kei and Tseng, Paul and Ji, Shuiwang and Ye, Jieping},
	month = jan,
	year = {2010},
	pages = {3465--3489},
}

@article{jaggi_simple_nodate-1,
	title = {A {Simple} {Algorithm} for {Nuclear} {Norm} {Regularized} {Problems}},
	abstract = {Optimization problems with a nuclear norm regularization, such as e.g. low norm matrix factorizations, have seen many applications recently. We propose a new approximation algorithm building upon the recent sparse approximate SDP solver of (Hazan, 2008). The experimental eﬃciency of our method is demonstrated on large matrix completion problems such as the Netﬂix dataset. The algorithm comes with strong convergence guarantees, and can be interpreted as a ﬁrst theoretically justiﬁed variant of Simon-Funk-type SVD heuristics. The method is free of tuning parameters, and very easy to parallelize.},
	language = {en},
	author = {Jaggi, Martin and Sulovský, Marek},
	pages = {8},
}

@article{razin_implicit_2020,
	title = {Implicit {Regularization} in {Deep} {Learning} {May} {Not} {Be} {Explainable} by {Norms}},
	url = {http://arxiv.org/abs/2005.06398},
	abstract = {Mathematically characterizing the implicit regularization induced by gradient-based optimization is a longstanding pursuit in the theory of deep learning. A widespread hope is that a characterization based on minimization of norms may apply, and a standard test-bed for studying this prospect is matrix factorization (matrix completion via linear neural networks). It is an open question whether norms can explain the implicit regularization in matrix factorization. The current paper resolves this open question in the negative, by proving that there exist natural matrix factorization problems on which the implicit regularization drives all norms (and quasi-norms) towards infinity. Our results suggest that, rather than perceiving the implicit regularization via norms, a potentially more useful interpretation is minimization of rank. We demonstrate empirically that this interpretation extends to a certain class of non-linear neural networks, and hypothesize that it may be key to explaining generalization in deep learning.},
	urldate = {2020-05-15},
	journal = {arXiv:2005.06398 [cs, stat]},
	author = {Razin, Noam and Cohen, Nadav},
	month = may,
	year = {2020},
	note = {arXiv: 2005.06398},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@incollection{hutchison_rank_2005,
	address = {Berlin, Heidelberg},
	title = {Rank, {Trace}-{Norm} and {Max}-{Norm}},
	volume = {3559},
	isbn = {978-3-540-26556-6 978-3-540-31892-7},
	url = {http://link.springer.com/10.1007/11503415_37},
	abstract = {We study the rank, trace-norm and max-norm as complexity measures of matrices, focusing on the problem of ﬁtting a matrix with matrices having low complexity. We present generalization error bounds for predicting unobserved entries that are based on these measures. We also consider the possible relations between these measures. We show gaps between them, and bounds on the extent of such gaps.},
	language = {en},
	urldate = {2020-05-14},
	booktitle = {Learning {Theory}},
	publisher = {Springer Berlin Heidelberg},
	author = {Srebro, Nathan and Shraibman, Adi},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Auer, Peter and Meir, Ron},
	year = {2005},
	doi = {10.1007/11503415_37},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {545--560},
}

@misc{noauthor_mail_nodate,
	title = {Mail - leello.dadi@epfl.ch},
	url = {https://ewa.epfl.ch/owa/#path=/mail},
	urldate = {2020-05-13},
}

@book{boyd_convex_2004,
	address = {Cambridge, UK ; New York},
	title = {Convex optimization},
	isbn = {978-0-521-83378-3},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Boyd, Stephen P. and Vandenberghe, Lieven},
	year = {2004},
	keywords = {Convex functions, Mathematical optimization},
}

@article{vishnoi_lx_2013,
	title = {Lx = b},
	volume = {8},
	issn = {1551-305X, 1551-3068},
	url = {http://www.nowpublishers.com/articles/foundations-and-trends-in-theoretical-computer-science/TCS-054},
	doi = {10.1561/0400000054},
	abstract = {The ability to solve a system of linear equations lies at the heart of areas such as optimization, scientiﬁc computing, and computer science, and has traditionally been a central topic of research in the area of numerical linear algebra. An important class of instances that arise in practice has the form Lx = b, where L is the Laplacian of an undirected graph. After decades of sustained research and combining tools from disparate areas, we now have Laplacian solvers that run in time nearlylinear in the sparsity (that is, the number of edges in the associated graph) of the system, which is a distant goal for general systems. Surprisingly, and perhaps not the original motivation behind this line of research, Laplacian solvers are impacting the theory of fast algorithms for fundamental graph problems. In this monograph, the emerging paradigm of employing Laplacian solvers to design novel fast algorithms for graph problems is illustrated through a small but carefully chosen set of examples. A part of this monograph is also dedicated to developing the ideas that go into the construction of near-linear-time Laplacian solvers. An understanding of these methods, which marry techniques from linear algebra and graph theory, will not only enrich the tool-set of an algorithm designer but will also provide the ability to adapt these methods to design fast algorithms for other fundamental problems.},
	language = {en},
	number = {1-2},
	urldate = {2020-05-12},
	journal = {Foundations and Trends® in Theoretical Computer Science},
	author = {Vishnoi, Nisheeth K.},
	year = {2013},
	pages = {1--141},
}

@article{bach_duality_2015,
	title = {Duality {Between} {Subgradient} and {Conditional} {Gradient} {Methods}},
	volume = {25},
	issn = {1052-6234},
	url = {https://epubs.siam.org/doi/abs/10.1137/130941961},
	doi = {10.1137/130941961},
	abstract = {Given a convex optimization problem and its dual, there are many possible first-order algorithms. In this paper, we show the equivalence between mirror descent algorithms and algorithms generalizing the conditional gradient method. This is done through convex duality and implies notably that for certain problems, such as for supervised machine learning problems with nonsmooth losses or problems regularized by nonsmooth regularizers, the primal subgradient method and the dual conditional gradient method are formally equivalent. The dual interpretation leads to a form of line search for mirror descent, as well as guarantees of convergence for primal-dual certificates.},
	number = {1},
	urldate = {2020-05-12},
	journal = {SIAM Journal on Optimization},
	author = {Bach, Francis},
	month = jan,
	year = {2015},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {115--129},
}

@article{udell_why_2019,
	title = {Why {Are} {Big} {Data} {Matrices} {Approximately} {Low} {Rank}?},
	volume = {1},
	url = {https://epubs.siam.org/doi/abs/10.1137/18M1183480},
	doi = {10.1137/18M1183480},
	abstract = {Matrices of (approximate) low rank are pervasive in data science, appearing in movie preferences, text documents, survey data, medical records, and genomics. While there is a vast literature on how to exploit low rank structure in these datasets, there is less attention paid to explaining why the low rank structure appears in the first place. Here, we explain the effectiveness of low rank models in data science by considering a simple generative model for these matrices: we suppose that each row or column is associated to a (possibly high dimensional) bounded latent variable, and entries of the matrix are generated by applying a piecewise analytic function to these latent variables. These matrices are in general full rank. However, we show that we can approximate every entry of an \$m{\textbackslash}times n\$ matrix drawn from this model to within a fixed absolute error by a low rank matrix whose rank grows as \${\textbackslash}mathcal\{O\}({\textbackslash}log(m+n))\$. Hence any sufficiently large matrix from such a latent variable model can be approximated, up to a small entrywise error, by a low rank matrix.},
	number = {1},
	urldate = {2020-05-12},
	journal = {SIAM Journal on Mathematics of Data Science},
	author = {Udell, Madeleine and Townsend, Alex},
	month = jan,
	year = {2019},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {144--160},
}

@article{nakkiran_deep_2019,
	title = {Deep {Double} {Descent}: {Where} {Bigger} {Models} and {More} {Data} {Hurt}},
	shorttitle = {Deep {Double} {Descent}},
	url = {http://arxiv.org/abs/1912.02292},
	abstract = {We show that a variety of modern deep learning tasks exhibit a "double-descent" phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.},
	urldate = {2020-05-12},
	journal = {arXiv:1912.02292 [cs, stat]},
	author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.02292},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{nelson_chaining_nodate,
	title = {Chaining introduction with some computer science applications},
	language = {en},
	author = {Nelson, Jelani},
	pages = {25},
}

@inproceedings{vempala_spectral_2007,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Spectral {Algorithms} for {Learning} and {Clustering}},
	isbn = {978-3-540-72927-3},
	doi = {10.1007/978-3-540-72927-3_2},
	abstract = {Roughly speaking, spectral algorithms are methods that rely on the principal components (typically singular values and singular vectors) of an input matrix (or graph). The spectrum of a matrix captures many interesting properties in surprising ways. Spectral methods are already used for unsupervised learning, image segmentation, to improve precision and recall in databases and broadly for information retrieval. The common component of these methods is the subspace of a small number of singular vectors of the data, by means of the Singular Value Decomposition (SVD). We describe SVD from a geometric perspective and then focus on its central role in efficient algorithms for (a) the classical problem of “learning” a mixture of Gaussians in Rn and (b) clustering a set of objects from pairwise similarities.},
	language = {en},
	booktitle = {Learning {Theory}},
	publisher = {Springer},
	author = {Vempala, Santosh S.},
	editor = {Bshouty, Nader H. and Gentile, Claudio},
	year = {2007},
	keywords = {Image Segmentation, Information Retrieval, Mixture Model, Singular Value Decomposition, Singular Vector},
	pages = {3--4},
}

@article{kannan_randomized_2017,
	title = {Randomized algorithms in numerical linear algebra},
	volume = {26},
	issn = {0962-4929, 1474-0508},
	url = {https://www.cambridge.org/core/product/identifier/S0962492917000058/type/journal_article},
	doi = {10.1017/S0962492917000058},
	abstract = {This survey provides an introduction to the use of randomization in the design of fast algorithms for numerical linear algebra. These algorithms typically examine only a subset of the input to solve basic problems approximately, including matrix multiplication, regression and low-rank approximation. The survey describes the key ideas and gives complete proofs of the main results in the field. A central unifying idea is sampling the columns (or rows) of a matrix according to their squared lengths.},
	language = {en},
	urldate = {2020-05-12},
	journal = {Acta Numerica},
	author = {Kannan, Ravindran and Vempala, Santosh},
	month = may,
	year = {2017},
	pages = {95--135},
}

@article{vempala_recent_nodate,
	title = {Recent {Progress} and {Open} {Problems} in {Algorithmic} {Convex} {Geometry}},
	abstract = {This article is a survey of developments in algorithmic convex geometry over the past decade. These include algorithms for sampling, optimization, integration, rounding and learning, as well as mathematical tools such as isoperimetric and concentration inequalities. Several open problems and conjectures are discussed on the way.},
	language = {en},
	author = {Vempala, Santosh S},
	pages = {24},
}

@article{chen_optimal_2019,
	title = {Optimal {Convergence} {Rate} of {Hamiltonian} {Monte} {Carlo} for {Strongly} {Logconcave} {Distributions}},
	url = {http://arxiv.org/abs/1905.02313},
	abstract = {We study Hamiltonian Monte Carlo (HMC) for sampling from a strongly logconcave density proportional to \$e{\textasciicircum}\{-f\}\$ where \$f:{\textbackslash}mathbb\{R\}{\textasciicircum}d {\textbackslash}to {\textbackslash}mathbb\{R\}\$ is \${\textbackslash}mu\$-strongly convex and \$L\$-smooth (the condition number is \${\textbackslash}kappa = L/{\textbackslash}mu\$). We show that the relaxation time (inverse of the spectral gap) of ideal HMC is \$O({\textbackslash}kappa)\$, improving on the previous best bound of \$O({\textbackslash}kappa{\textasciicircum}\{1.5\})\$; we complement this with an example where the relaxation time is \${\textbackslash}Omega({\textbackslash}kappa)\$. When implemented using a nearly optimal ODE solver, HMC returns an \${\textbackslash}varepsilon\$-approximate point in \$2\$-Wasserstein distance using \${\textbackslash}widetilde\{O\}(({\textbackslash}kappa d){\textasciicircum}\{0.5\} {\textbackslash}varepsilon{\textasciicircum}\{-1\})\$ gradient evaluations per step and \${\textbackslash}widetilde\{O\}(({\textbackslash}kappa d){\textasciicircum}\{1.5\}{\textbackslash}varepsilon{\textasciicircum}\{-1\})\$ total time.},
	urldate = {2020-05-12},
	journal = {arXiv:1905.02313 [cs, stat]},
	author = {Chen, Zongchen and Vempala, Santosh S.},
	month = may,
	year = {2019},
	note = {arXiv: 1905.02313},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{arora_fine-grained_2019,
	title = {Fine-{Grained} {Analysis} of {Optimization} and {Generalization} for {Overparameterized} {Two}-{Layer} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1901.08584},
	abstract = {Recent works have cast some light on the mystery of why deep nets fit any data and generalize despite being very overparametrized. This paper analyzes training and generalization for a simple 2-layer ReLU net with random initialization, and provides the following improvements over recent works: (i) Using a tighter characterization of training speed than recent papers, an explanation for why training a neural net with random labels leads to slower training, as originally observed in [Zhang et al. ICLR'17]. (ii) Generalization bound independent of network size, using a data-dependent complexity measure. Our measure distinguishes clearly between random labels and true labels on MNIST and CIFAR, as shown by experiments. Moreover, recent papers require sample complexity to increase (slowly) with the size, while our sample complexity is completely independent of the network size. (iii) Learnability of a broad class of smooth functions by 2-layer ReLU nets trained via gradient descent. The key idea is to track dynamics of training and generalization via properties of a related kernel.},
	urldate = {2020-05-12},
	journal = {arXiv:1901.08584 [cs, stat]},
	author = {Arora, Sanjeev and Du, Simon S. and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
	month = may,
	year = {2019},
	note = {arXiv: 1901.08584},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{weed_estimation_2019,
	title = {Estimation of smooth densities in {Wasserstein} distance},
	abstract = {The Wasserstein distances are a set of metrics on probability distributions supported on \${\textbackslash}mathbb\{R\}{\textasciicircum}d\$ with applications throughout statistics and machine learning. Often, such distances are used in the context of variational problems, in which the statistician employs in place of an unknown measure a proxy constructed on the basis of independent samples. This raises the basic question of how well measures can be approximated in Wasserstein distance. While it is known that an empirical measure comprising i.i.d. samples is rate-optimal for general measures, no improved results were known for measures possessing smooth densities. We prove the first minimax rates for estimation of smooth densities for general Wasserstein distances, thereby showing how the curse of dimensionality can be alleviated for sufficiently regular measures. We also show how to construct discretely supported measures, suitable for computational purposes, which enjoy improved rates. Our approach is based on novel bounds between the Wasserstein distances and suitable Besov norms, which may be of independent interest.},
	booktitle = {{COLT}},
	author = {Weed, Jonathan and Berthet, Quentin},
	year = {2019},
}

@article{karras_analyzing_2020,
	title = {Analyzing and {Improving} the {Image} {Quality} of {StyleGAN}},
	url = {http://arxiv.org/abs/1912.04958},
	abstract = {The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.},
	urldate = {2020-05-11},
	journal = {arXiv:1912.04958 [cs, eess, stat]},
	author = {Karras, Tero and Laine, Samuli and Aittala, Miika and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
	month = mar,
	year = {2020},
	note = {arXiv: 1912.04958},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Electrical Engineering and Systems Science - Image and Video Processing, Statistics - Machine Learning},
}

@article{dalalyan_theoretical_2016,
	title = {Theoretical guarantees for approximate sampling from smooth and log-concave densities},
	url = {http://arxiv.org/abs/1412.7392},
	abstract = {Sampling from various kinds of distributions is an issue of paramount importance in statistics since it is often the key ingredient for constructing estimators, test procedures or confidence intervals. In many situations, the exact sampling from a given distribution is impossible or computationally expensive and, therefore, one needs to resort to approximate sampling strategies. However, there is no well-developed theory providing meaningful nonasymptotic guarantees for the approximate sampling procedures, especially in the high-dimensional problems. This paper makes some progress in this direction by considering the problem of sampling from a distribution having a smooth and log-concave density defined on {\textbackslash}({\textbackslash}RR{\textasciicircum}p{\textbackslash}), for some integer {\textbackslash}(p{\textgreater}0{\textbackslash}). We establish nonasymptotic bounds for the error of approximating the target distribution by the one obtained by the Langevin Monte Carlo method and its variants. We illustrate the effectiveness of the established guarantees with various experiments. Underlying our analysis are insights from the theory of continuous-time diffusion processes, which may be of interest beyond the framework of log-concave densities considered in the present work.},
	urldate = {2020-05-11},
	journal = {arXiv:1412.7392 [math, stat]},
	author = {Dalalyan, Arnak S.},
	month = dec,
	year = {2016},
	note = {arXiv: 1412.7392},
	keywords = {Mathematics - Statistics Theory, Statistics - Computation, Statistics - Machine Learning},
}

@article{lee_faster_nodate,
	title = {Faster {Algorithms} for {Convex} and {Combinatorial} {Optimization}},
	language = {en},
	author = {Lee, Yin Tat},
	pages = {458},
}

@misc{noauthor_consulat_nodate,
	title = {Consulat général d'Éthiopie à {Genève}, {Suisse}},
	url = {https://www.embassypages.com/ethiopie-consulatgeneral-geneve-suisse},
	urldate = {2020-03-23},
}

@misc{noauthor_authentication_nodate,
	title = {Authentication page},
	url = {https://loginfmel.world-connect.ch/401.php?msg=bm9fcG9ydGFsX3ZpYV9pZmFjZQ==},
	urldate = {2020-03-18},
}

@article{comon_symmetric_2008,
	title = {Symmetric {Tensors} and {Symmetric} {Tensor} {Rank}},
	volume = {30},
	issn = {0895-4798, 1095-7162},
	url = {http://epubs.siam.org/doi/10.1137/060661569},
	doi = {10.1137/060661569},
	abstract = {A symmetric tensor is a higher order generalization of a symmetric matrix. In this paper, we study various properties of symmetric tensors in relation to a decomposition into a symmetric sum of outer product of vectors. A rank-1 order-k tensor is the outer product of k non-zero vectors. Any symmetric tensor can be decomposed into a linear combination of rank-1 tensors, each of them being symmetric or not. The rank of a symmetric tensor is the minimal number of rank-1 tensors that is necessary to reconstruct it. The symmetric rank is obtained when the constituting rank-1 tensors are imposed to be themselves symmetric. It is shown that rank and symmetric rank are equal in a number of cases, and that they always exist in an algebraically closed ﬁeld. We will discuss the notion of the generic symmetric rank, which, due to the work of Alexander and Hirschowitz, is now known for any values of dimension and order. We will also show that the set of symmetric tensors of symmetric rank at most r is not closed, unless r = 1.},
	language = {en},
	number = {3},
	urldate = {2020-03-12},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	author = {Comon, Pierre and Golub, Gene and Lim, Lek-Heng and Mourrain, Bernard},
	month = jan,
	year = {2008},
	pages = {1254--1279},
}

@article{spielman_smoothed_2004,
	title = {Smoothed analysis of algorithms: {Why} the simplex algorithm usually takes polynomial time},
	volume = {51},
	issn = {0004-5411},
	shorttitle = {Smoothed analysis of algorithms},
	url = {https://doi.org/10.1145/990308.990310},
	doi = {10.1145/990308.990310},
	abstract = {We introduce the smoothed analysis of algorithms, which continuously interpolates between the worst-case and average-case analyses of algorithms. In smoothed analysis, we measure the maximum over inputs of the expected performance of an algorithm under small random perturbations of that input. We measure this performance in terms of both the input size and the magnitude of the perturbations. We show that the simplex algorithm has smoothed complexity polynomial in the input size and the standard deviation of Gaussian perturbations.},
	number = {3},
	urldate = {2020-03-03},
	journal = {Journal of the ACM},
	author = {Spielman, Daniel A. and Teng, Shang-Hua},
	month = may,
	year = {2004},
	keywords = {Simplex method, complexity, perturbation, smoothed analysis},
	pages = {385--463},
}

@inproceedings{stuhlsatz_making_2008,
	title = {Making the {Lipschitz} {Classifier} {Practical} via {Semi}-infinite {Programming}},
	doi = {10.1109/ICMLA.2008.26},
	abstract = {This paper presents a new implementable algorithm for solving the Lipschitz classifier that is a generalization of the maximum margin concept from Hilbert to Banach spaces. In contrast to the support vector machine approach, our algorithm is free to use any finite family of continuously differentiable functions which linearly compose the decision function. Nevertheless, robustness properties are maintained due to a maximizing margin. To obtain a useful algorithm, the inherent difficult problem is formulated in a convex semi-infinite program. Using this new formulation, we develop a duality result enabling us to solve the original problem iteratively as a finite sequence of constrained quadratic programming problems over a convex hull of matrices. We compare the performance of the Lipschitz classifier algorithm with state-of-the-art machine learning methodologies using a benchmark data set as well as a data set randomly generated from Gaussian mixtures.},
	booktitle = {2008 {Seventh} {International} {Conference} on {Machine} {Learning} and {Applications}},
	author = {Stuhlsatz, André and Meier, Hans-Günter and Wendemuth, Andreas},
	month = dec,
	year = {2008},
	note = {ISSN: null},
	keywords = {Banach space, Banach spaces, Extraterrestrial measurements, Hilbert space, Hilbert spaces, Iterative algorithms, Least squares approximation, Lipschitz classifier, Machine learning, Machine learning algorithms, Robustness, SIP, SVM, Support vector machine classification, Support vector machines, Training data, constrained quadratic programming, convex SIP, convex programming, convex semiinfinite programming, duality, machine learning, maximum margin concept, maxmimum margin, pattern classification, semi-infinite programming, support vector maschine},
	pages = {40--47},
}

@incollection{kacprzyk_maximum_2007,
	address = {Berlin, Heidelberg},
	title = {Maximum {Margin} {Classification} on {Convex} {Euclidean} {Metric} {Spaces}},
	volume = {45},
	isbn = {978-3-540-75174-8 978-3-540-75175-5},
	url = {http://link.springer.com/10.1007/978-3-540-75175-5_27},
	abstract = {In this paper, we present a new implementable learning algorithm for the general nonlinear binary classiﬁcation problem. The suggested algorithm abides the maximum margin philosophy, and learns a decision function from the set of all ﬁnite linear combinations of continuous diﬀerentiable basis functions. This enables the use of a much more ﬂexible function class than the one usually employed by Mercer-restricted kernel machines. Experiments on 2-dimensional randomly generated data are given to compare the algorithm to a Support Vector Machine. While the performances are comparable in case of Gaussian basis functions and static feature vectors the algorithm opens a novel way to hitherto intractable problems. This includes especially classiﬁcation of feature vector streams, or features with dynamically varying dimensions as such in DNA analysis, natural speech or motion image recognition.},
	language = {en},
	urldate = {2020-02-26},
	booktitle = {Computer {Recognition} {Systems} 2},
	publisher = {Springer Berlin Heidelberg},
	author = {Stuhlsatz, André and Meier, Hans-Günter and Wendemuth, Andreas},
	editor = {Kacprzyk, Janusz and Kurzynski, Marek and Puchala, Edward and Wozniak, Michal and Zolnierek, Andrzej},
	year = {2007},
	doi = {10.1007/978-3-540-75175-5_27},
	pages = {216--223},
}

@article{trafalis_robust_2006,
	title = {Robust classification and regression using support vector machines},
	volume = {173},
	issn = {0377-2217},
	url = {http://www.sciencedirect.com/science/article/pii/S0377221705006892},
	doi = {10.1016/j.ejor.2005.07.024},
	abstract = {In this paper, we investigate the theoretical aspects of robust classification and robust regression using support vector machines. Given training data (x1,y1),…,(xl,yl), where l represents the number of samples, xi∈Rn and yi∈\{−1,1\} (for classification) or yi∈R (for regression), we investigate the training of a support vector machine in the case where bounded perturbation is added to the value of the input xi∈Rn. We consider both cases where our training data are either linearly separable and nonlinearly separable respectively. We show that we can perform robust classification or regression by using linear or second order cone programming.},
	language = {en},
	number = {3},
	urldate = {2020-02-25},
	journal = {European Journal of Operational Research},
	author = {Trafalis, Theodore B. and Gilbert, Robin C.},
	month = sep,
	year = {2006},
	keywords = {Classification, Regression, Robustness, Support vector machines},
	pages = {893--909},
}

@article{mcshane_extension_1934,
	title = {Extension of range of functions},
	volume = {40},
	issn = {0002-9904, 1936-881X},
	url = {https://www.ams.org/bull/1934-40-12/S0002-9904-1934-05978-0/},
	doi = {10.1090/S0002-9904-1934-05978-0},
	abstract = {Advancing research. Creating connections.},
	language = {en},
	number = {12},
	urldate = {2020-02-06},
	journal = {Bulletin of the American Mathematical Society},
	author = {McShane, E. J.},
	year = {1934},
	pages = {837--842},
}

@article{luxburg_distance-based_2004,
	title = {Distance-{Based} {Classification} with {Lipschitz} {Functions}},
	volume = {5},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v5/luxburg04b.html},
	number = {Jun},
	urldate = {2020-02-06},
	journal = {Journal of Machine Learning Research},
	author = {Luxburg, Ulrike von and Bousquet, Olivier},
	year = {2004},
	pages = {669--695},
}

@article{bubeck_convex_2015,
	title = {Convex {Optimization}: {Algorithms} and {Complexity}},
	shorttitle = {Convex {Optimization}},
	url = {http://arxiv.org/abs/1405.4980},
	abstract = {This monograph presents the main complexity theorems in convex optimization and their corresponding algorithms. Starting from the fundamental theory of black-box optimization, the material progresses towards recent advances in structural optimization and stochastic optimization. Our presentation of black-box optimization, strongly influenced by Nesterov's seminal book and Nemirovski's lecture notes, includes the analysis of cutting plane methods, as well as (accelerated) gradient descent schemes. We also pay special attention to non-Euclidean settings (relevant algorithms include Frank-Wolfe, mirror descent, and dual averaging) and discuss their relevance in machine learning. We provide a gentle introduction to structural optimization with FISTA (to optimize a sum of a smooth and a simple non-smooth term), saddle-point mirror prox (Nemirovski's alternative to Nesterov's smoothing), and a concise description of interior point methods. In stochastic optimization we discuss stochastic gradient descent, mini-batches, random coordinate descent, and sublinear algorithms. We also briefly touch upon convex relaxation of combinatorial problems and the use of randomness to round solutions, as well as random walks based methods.},
	urldate = {2020-02-05},
	journal = {arXiv:1405.4980 [cs, math, stat]},
	author = {Bubeck, Sébastien},
	month = nov,
	year = {2015},
	note = {arXiv: 1405.4980},
	keywords = {Computer Science - Computational Complexity, Computer Science - Machine Learning, Mathematics - Numerical Analysis, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@book{vershynin_high-dimensional_2018,
	edition = {1},
	title = {High-{Dimensional} {Probability}: {An} {Introduction} with {Applications} in {Data} {Science}},
	isbn = {978-1-108-23159-6 978-1-108-41519-4},
	shorttitle = {High-{Dimensional} {Probability}},
	url = {https://www.cambridge.org/core/product/identifier/9781108231596/type/book},
	language = {en},
	urldate = {2020-02-05},
	publisher = {Cambridge University Press},
	author = {Vershynin, Roman},
	month = sep,
	year = {2018},
	doi = {10.1017/9781108231596},
}

@article{woodruff_sketching_2014,
	title = {Sketching as a {Tool} for {Numerical} {Linear} {Algebra}},
	volume = {10},
	issn = {1551-305X, 1551-3068},
	url = {http://arxiv.org/abs/1411.4357},
	doi = {10.1561/0400000060},
	abstract = {This survey highlights the recent advances in algorithms for numerical linear algebra that have come from the technique of linear sketching, whereby given a matrix, one first compresses it to a much smaller matrix by multiplying it by a (usually) random matrix with certain properties. Much of the expensive computation can then be performed on the smaller matrix, thereby accelerating the solution for the original problem. In this survey we consider least squares as well as robust regression problems, low rank approximation, and graph sparsification. We also discuss a number of variants of these problems. Finally, we discuss the limitations of sketching methods.},
	number = {1-2},
	urldate = {2020-02-05},
	journal = {Foundations and Trends® in Theoretical Computer Science},
	author = {Woodruff, David P.},
	year = {2014},
	note = {arXiv: 1411.4357},
	keywords = {Computer Science - Data Structures and Algorithms},
	pages = {1--157},
}

@article{lemon_low-rank_2016,
	title = {Low-{Rank} {Semidefinite} {Programming}: {Theory} and {Applications}},
	volume = {2},
	issn = {2167-3888, 2167-3918},
	shorttitle = {Low-{Rank} {Semidefinite} {Programming}},
	url = {http://www.nowpublishers.com/article/Details/OPT-009},
	doi = {10.1561/2400000009},
	abstract = {Finding low-rank solutions of semideﬁnite programs is important in many applications. For example, semideﬁnite programs that arise as relaxations of polynomial optimization problems are exact relaxations when the semideﬁnite program has a rank-1 solution. Unfortunately, computing a minimum-rank solution of a semideﬁnite program is an NP-hard problem. In this paper we review the theory of low-rank semideﬁnite programming, presenting theorems that guarantee the existence of a low-rank solution, heuristics for computing low-rank solutions, and algorithms for ﬁnding low-rank approximate solutions. Then we present applications of the theory to trust-region problems and signal processing.},
	language = {en},
	number = {1-2},
	urldate = {2020-02-05},
	journal = {Foundations and Trends® in Optimization},
	author = {Lemon, Alex and So, Anthony Man-Cho and Ye, Yinyu},
	year = {2016},
	pages = {1--156},
}

@article{mahoney_randomized_2011,
	title = {Randomized algorithms for matrices and data},
	url = {http://arxiv.org/abs/1104.5557},
	abstract = {Randomized algorithms for very large matrix problems have received a great deal of attention in recent years. Much of this work was motivated by problems in large-scale data analysis, and this work was performed by individuals from many different research communities. This monograph will provide a detailed overview of recent work on the theory of randomized matrix algorithms as well as the application of those ideas to the solution of practical problems in large-scale data analysis. An emphasis will be placed on a few simple core ideas that underlie not only recent theoretical advances but also the usefulness of these tools in large-scale data applications. Crucial in this context is the connection with the concept of statistical leverage. This concept has long been used in statistical regression diagnostics to identify outliers; and it has recently proved crucial in the development of improved worst-case matrix algorithms that are also amenable to high-quality numerical implementation and that are useful to domain scientists. Randomized methods solve problems such as the linear least-squares problem and the low-rank matrix approximation problem by constructing and operating on a randomized sketch of the input matrix. Depending on the specifics of the situation, when compared with the best previously-existing deterministic algorithms, the resulting randomized algorithms have worst-case running time that is asymptotically faster; their numerical implementations are faster in terms of clock-time; or they can be implemented in parallel computing environments where existing numerical algorithms fail to run at all. Numerous examples illustrating these observations will be described in detail.},
	urldate = {2020-02-05},
	journal = {arXiv:1104.5557 [cs]},
	author = {Mahoney, Michael W.},
	month = nov,
	year = {2011},
	note = {arXiv: 1104.5557},
	keywords = {Computer Science - Data Structures and Algorithms},
}

@article{xu_robustness_2009,
	title = {Robustness and {Regularization} of {Support} {Vector} {Machines}},
	volume = {10},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v10/xu09b.html},
	number = {Jul},
	urldate = {2020-02-05},
	journal = {Journal of Machine Learning Research},
	author = {Xu, Huan and Caramanis, Constantine and Mannor, Shie},
	year = {2009},
	pages = {1485--1510},
}

@article{wilson_marginal_nodate,
	title = {The {Marginal} {Value} of {Adaptive} {Gradient} {Methods} in {Machine} {Learning}},
	abstract = {Adaptive optimization methods, which perform local optimization with a metric constructed from the history of iterates, are becoming increasingly popular for training deep neural networks. Examples include AdaGrad, RMSProp, and Adam. We show that for simple overparameterized problems, adaptive methods often ﬁnd drastically diﬀerent solutions than gradient descent (GD) or stochastic gradient descent (SGD). We construct an illustrative binary classiﬁcation problem where the data is linearly separable, GD and SGD achieve zero test error, and AdaGrad, Adam, and RMSProp attain test errors arbitrarily close to half. We additionally study the empirical generalization capability of adaptive methods on several state-of-the-art deep learning models. We observe that the solutions found by adaptive methods generalize worse (often signiﬁcantly worse) than SGD, even when these solutions have better training performance. These results suggest that practitioners should reconsider the use of adaptive methods to train neural networks.},
	language = {en},
	author = {Wilson, Ashia C and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nathan and Recht, Benjamin},
	pages = {14},
}

@article{levy_necessary_2019,
	title = {Necessary and {Sufficient} {Geometries} for {Gradient} {Methods}},
	url = {http://arxiv.org/abs/1909.10455},
	abstract = {We study the impact of the constraint set and gradient geometry on the convergence of online and stochastic methods for convex optimization, providing a characterization of the geometries for which stochastic gradient and adaptive gradient methods are (minimax) optimal. In particular, we show that when the constraint set is quadratically convex, diagonally pre-conditioned stochastic gradient methods are minimax optimal. We further provide a converse that shows that when the constraints are not quadratically convex---for example, any \${\textbackslash}ell\_p\$-ball for \$p {\textless} 2\$---the methods are far from optimal. Based on this, we can provide concrete recommendations for when one should use adaptive, mirror or stochastic gradient methods.},
	urldate = {2020-02-03},
	journal = {arXiv:1909.10455 [cs, math, stat]},
	author = {Levy, Daniel and Duchi, John C.},
	month = oct,
	year = {2019},
	note = {arXiv: 1909.10455},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{shah_minimum_2018,
	title = {Minimum norm solutions do not always generalize well for over-parameterized problems},
	volume = {abs/1811.07055},
	abstract = {Stochastic gradient descent is the de facto algorithm for training deep neural networks (DNNs). Despite its popularity, it still requires fine tuning in order to achieve its best performance. This has led to the development of adaptive methods, that claim automatic hyper-parameter optimization. 
Recently, researchers have studied both algorithmic classes via toy examples: e.g., for over-parameterized linear regression, Wilson et. al. (2017) shows that, while SGD always converges to the minimum-norm solution, adaptive methods show no such inclination, leading to worse generalization capabilities. 
Our aim is to study this conjecture further. We empirically show that the minimum weight norm is not necessarily the proper gauge of good generalization in simplified scenaria, and different models found by adaptive methods could outperform plain gradient methods. In practical DNN settings, we observe that adaptive methods can outperform SGD, with larger weight norm output models, but without necessarily reducing the amount of tuning required.},
	journal = {ArXiv},
	author = {Shah, Vatsal and Kyrillidis, Anastasios and Sanghavi, Sujay},
	year = {2018},
	keywords = {Algorithm, Artificial neural network, Deep learning, Generalization (Psychology), Hyperactive behavior, Large, Mathematical optimization, Minimum weight, Population Parameter, Stochastic gradient descent, Toy Manchester Terrier},
}

@incollection{qian_implicit_2019,
	title = {The {Implicit} {Bias} of {AdaGrad} on {Separable} {Data}},
	url = {http://papers.nips.cc/paper/8991-the-implicit-bias-of-adagrad-on-separable-data.pdf},
	urldate = {2020-01-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Qian, Qian and Qian, Xiaoyuan},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {7759--7767},
}

@incollection{awasthi_robustness_2019,
	title = {On {Robustness} to {Adversarial} {Examples} and {Polynomial} {Optimization}},
	url = {http://papers.nips.cc/paper/9526-on-robustness-to-adversarial-examples-and-polynomial-optimization.pdf},
	urldate = {2020-01-24},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Awasthi, Pranjal and Dutta, Abhratanu and Vijayaraghavan, Aravindan},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {13737--13747},
}

@article{bertsimas_robust_2018,
	title = {Robust {Classification}},
	volume = {1},
	issn = {2575-1484},
	url = {https://pubsonline.informs.org/doi/abs/10.1287/ijoo.2018.0001},
	doi = {10.1287/ijoo.2018.0001},
	abstract = {Motivated by the fact that there may be inaccuracies in features and labels of training data, we apply robust optimization techniques to study in a principled way the uncertainty in data features and labels in classification problems and obtain robust formulations for the three most widely used classification methods: support vector machines, logistic regression, and decision trees. We show that adding robustness does not materially change the complexity of the problem and that all robust counterparts can be solved in practical computational times. We demonstrate the advantage of these robust formulations over regularized and nominal methods in synthetic data experiments, and we show that our robust classification methods offer improved out-of-sample accuracy. Furthermore, we run large-scale computational experiments across a sample of 75 data sets from the University of California Irvine Machine Learning Repository and show that adding robustness to any of the three nonregularized classification methods improves the accuracy in the majority of the data sets. We observe the most significant gains for robust classification methods on high-dimensional and difficult classification problems, with an average improvement in out-of-sample accuracy of robust versus nominal problems of 5.3\% for support vector machines, 4.0\% for logistic regression, and 1.3\% for decision trees.},
	number = {1},
	urldate = {2020-01-24},
	journal = {INFORMS Journal on Optimization},
	author = {Bertsimas, Dimitris and Dunn, Jack and Pawlowski, Colin and Zhuo, Ying Daisy},
	month = oct,
	year = {2018},
	pages = {2--34},
}

@inproceedings{stuhlsatz_maximum_2007,
	address = {Berlin, Heidelberg},
	series = {Advances in {Soft} {Computing}},
	title = {Maximum {Margin} {Classification} on {Convex} {Euclidean} {Metric} {Spaces}},
	isbn = {978-3-540-75175-5},
	doi = {10.1007/978-3-540-75175-5_27},
	abstract = {In this paper, we present a new implementable learning algorithm for the general nonlinear binary classification problem. The suggested algorithm abides the maximum margin philosophy, and learns a decision function from the set of all finite linear combinations of continuous differentiable basis functions. This enables the use of a much more flexible function class than the one usually employed by Mercer-restricted kernel machines. Experiments on 2-dimensional randomly generated data are given to compare the algorithm to a Support Vector Machine. While the performances are comparable in case of Gaussian basis functions and static feature vectors the algorithm opens a novel way to hitherto intractable problems. This includes especially classification of feature vector streams, or features with dynamically varying dimensions as such in DNA analysis, natural speech or motion image recognition.},
	language = {en},
	booktitle = {Computer {Recognition} {Systems} 2},
	publisher = {Springer},
	author = {Stuhlsatz, André and Meier, Hans-Günter and Wendemuth, Andreas},
	editor = {Kurzynski, Marek and Puchala, Edward and Wozniak, Michal and Zolnierek, Andrzej},
	year = {2007},
	keywords = {Decision Function, Gaussian Basis Function, Maximum Margin, Natural Speech, Support Vector Machine},
	pages = {216--223},
}

@incollection{scholkopf_pattern_2001,
	title = {Pattern {Recognition}},
	url = {https://ieeexplore.ieee.org/document/6282662},
	abstract = {This chapter contains sections titled: Separating Hyperplanes, The Role of the Margin, Optimal Margin Hyperplanes, Nonlinear Support Vector Classifiers, Soft Margin Hyperplanes, Multi-Class Classification, Variations on a Theme, Experiments, Summary, Problems},
	urldate = {2020-01-16},
	booktitle = {Learning with {Kernels}: {Support} {Vector} {Machines}, {Regularization}, {Optimization}, and {Beyond}},
	publisher = {MITP},
	author = {Schölkopf, Bernhard and Smola, Alexander J.},
	year = {2001},
	note = {ISSN: null},
	pages = {189--226},
}

@misc{noauthor_160708810_nodate,
	title = {[1607.08810] {Polynomial} {Networks} and {Factorization} {Machines}: {New} {Insights} and {Efficient} {Training} {Algorithms}},
	url = {https://arxiv.org/abs/1607.08810},
	urldate = {2020-01-14},
}

@article{trafalis_robust_2007,
	title = {Robust support vector machines for classification and computational issues},
	volume = {22},
	issn = {1055-6788},
	url = {https://doi.org/10.1080/10556780600883791},
	doi = {10.1080/10556780600883791},
	abstract = {In this paper, we investigate the theoretical and numerical aspects of robust classification using support vector machines (SVMs) by providing second order cone programming and linear programming formulations. SVMs are learning algorithms introduced by Vapnik used either for classification or regression. They show good generalization properties and they are based on statistical learning theory. The resulting learning problems are convex optimization problems suitable for application of primal-dual interior points methods. We investigate the training of a SVM in the case where a bounded perturbation is added to the value of an input x i ∊ℝ n . A robust SVM provides a decision function that is immune to data perturbations. We consider both cases where our training data are either linearly separable or non linearly separable respectively and provide computational results for real data sets.},
	number = {1},
	urldate = {2020-01-13},
	journal = {Optimization Methods and Software},
	author = {Trafalis, T. B. and Gilbert, R. C.},
	month = feb,
	year = {2007},
	keywords = {Robust classification, Robust support vector machines, Support vector machines},
	pages = {187--198},
}

@article{forghaniyahya_comment_2013,
	title = {Comment on "{Robustness} and regularization of support vector machines" by {H}. {Xu} et al. ({Journal} of machine learning research, vol. 10, pp. 1485-1510, 2009)},
	url = {https://dl.acm.org/doi/abs/10.5555/2567709.2567774},
	abstract = {This paper comments on the published work dealing with robustness and regularization of support vector machines (Journal of Machine Learning Research, Vol. 10, pp. 1485-1510, 2009) by H. Xu et al. ...},
	language = {EN},
	urldate = {2020-01-13},
	journal = {The Journal of Machine Learning Research},
	author = {ForghaniYahya and SadoghiHadi},
	month = jan,
	year = {2013},
}

@misc{noauthor_ieee_nodate,
	title = {{IEEE} {Xplore} {Book} {Abstract} - {Learning} with {Kernels}: {Support} {Vector} {Machines}, {Regularization}, {Optimization}, and {Beyond}},
	url = {https://ieeexplore.ieee.org/book/6267332},
	urldate = {2020-01-13},
}

@article{akaho_maximing_2002,
	title = {Maximing the {Margin} in the {Input} {Space}},
	volume = {cs.AI/0211006},
	abstract = {We propose a novel criterion for support vector machine learning: maximizing the margin in the input space, not in the feature (Hilbert) space. This criterion is a discriminative version of the principal curve proposed by Hastie et al. The criterion is appropriate in particular when the input space is already a well-designed feature space with rather small dimensionality. The definition of the margin is generalized in order to represent prior knowledge. The derived algorithm consists of two alternating steps to estimate the dual parameters. Firstly, the parameters are initialized by the original SVM. Then one set of parameters is updated by Newton-like procedure, and the other set is updated by solving a quadratic programming problem. The algorithm converges in a few steps to a local optimum under mild conditions and it preserves the sparsity of support vectors. Although the complexity to calculate temporal variables increases the complexity to solve the quadratic programming problem for each step does not change. It is also shown that the original SVM can be seen as a special case. We further derive a simplified algorithm which enables us to use the existing code for the original SVM.},
	journal = {ArXiv},
	author = {Akaho, Shotaro},
	year = {2002},
	keywords = {Algorithm, Calculus of variations, Expect, Expectation–maximization algorithm, Feature vector, Hilbert space, Local optimum, Machine learning, Margin (machine learning), Mathematical optimization, Newton, Quadratic programming, Sparse matrix, Support vector machine},
}

@incollection{elsayed_large_2018,
	title = {Large {Margin} {Deep} {Networks} for {Classification}},
	url = {http://papers.nips.cc/paper/7364-large-margin-deep-networks-for-classification.pdf},
	urldate = {2020-01-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Elsayed, Gamaleldin and Krishnan, Dilip and Mobahi, Hossein and Regan, Kevin and Bengio, Samy},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {842--852},
}

@misc{noauthor_pii_nodate,
	title = {{PII}: {S0893}-6080(98)00032-{X} {\textbar} {Elsevier} {Enhanced} {Reader}},
	shorttitle = {{PII}},
	url = {https://reader.elsevier.com/reader/sd/pii/S089360809800032X?token=5C7C99860308E723FE0E58F5068FBD60C510D69F130FD1FA2CBA1E9D2379B336791DFA7736B523182FBF4C5EEEF6A9F9},
	language = {en},
	urldate = {2020-01-10},
	doi = {10.1016/S0893-6080(98)00032-X},
}

@book{scholkopf_support_1997,
	title = {Support {Vector} {Learning}},
	author = {Schölkopf, Bernhard},
	year = {1997},
}

@article{gottlieb_efficient_2014,
	title = {Efficient {Classification} for {Metric} {Data}},
	volume = {60},
	issn = {1557-9654},
	doi = {10.1109/TIT.2014.2339840},
	abstract = {Recent advances in large-margin classification of data residing in general metric spaces (rather than Hilbert spaces) enable classification under various natural metrics, such as string edit and earthmover distance. A general framework developed for this purpose left open the questions of computational efficiency and of providing direct bounds on generalization error. We design a new algorithm for classification in general metric spaces, whose runtime and accuracy depend on the doubling dimension of the data points, and can thus achieve superior classification performance in many common scenarios. The algorithmic core of our approach is an approximate (rather than exact) solution to the classical problems of Lipschitz extension and of nearest neighbor search. The algorithm's generalization performance is guaranteed via the fat-shattering dimension of Lipschitz classifiers, and we present experimental evidence of its superiority to some common kernel methods. As a by-product, we offer a new perspective on the nearest neighbor classifier, which yields significantly sharper risk asymptotics than the classic analysis.},
	number = {9},
	journal = {IEEE Transactions on Information Theory},
	author = {Gottlieb, Lee-Ad and Kontorovich, Aryeh and Krauthgamer, Robert},
	month = sep,
	year = {2014},
	keywords = {Algorithm design and analysis, Approximation algorithms, Classification, Extraterrestrial measurements, Hilbert space, Lipschitz classifiers, Lipschitz extension, Lipschitz function, Nearest neighbor searches, Training, computational efficiency, data handling, data points, doubling dimension, earthmover distance, fat-shattering dimension, general metric spaces, generalization error, large-margin data classification, metric data, metric space, nearest neighbor search, pattern classification, risk asymptotics, string edit},
	pages = {5750--5759},
}

@article{putinar_positive_1993,
	title = {Positive {Polynomials} on {Compact} {Semi}-algebraic {Sets}},
	volume = {42},
	issn = {0022-2518},
	url = {www.jstor.org/stable/24897130},
	number = {3},
	urldate = {2020-01-09},
	journal = {Indiana University Mathematics Journal},
	author = {Putinar, Mihai},
	year = {1993},
	pages = {969--984},
}

@article{liang_separating_2009,
	title = {Separating hypersurfaces of {SVMs} in input spaces},
	volume = {30},
	issn = {0167-8655},
	url = {http://www.sciencedirect.com/science/article/pii/S0167865508003644},
	doi = {10.1016/j.patrec.2008.12.002},
	abstract = {It is well-known that the separating hyperplane given by a (standard) support vector machine (SVM) is located in the middle of the margin with equal distance from the support vectors of the partitioned two clusters in the high-dimensional feature space. Whereas we expect that the corresponding separating hypersurface is also located in the middle of the margin with equal distance from the two clusters in the input sample space, in reality, it is not. We illustrate that in theory, the above “middle–located–hypersurface” expectation in input sample spaces is not ideally supported by SVMs. A few illustrative examples and additional experiments on large data sets are correspondingly investigated.},
	language = {en},
	number = {5},
	urldate = {2020-01-08},
	journal = {Pattern Recognition Letters},
	author = {Liang, Xun and Wang, Chao},
	month = apr,
	year = {2009},
	keywords = {High-dimensional feature space, Input sample space, Separating hyperplane, Separating hypersurface, Support vector machine},
	pages = {469--476},
}

@inproceedings{akaho_svm_2002,
	title = {{SVM} maximizing margin in the input space},
	volume = {2},
	doi = {10.1109/ICONIP.2002.1198224},
	abstract = {We propose a new type of support vector machine (SVM) that maximizes the margin in the input space, not in the feature space. Parameters are initialized by the original SVM, and they are updated by solving a quadratic programming problem iteratively. The derived algorithm preserves the sparsity of support vectors. It is also shown that the original SVM can be seen as a special case. The algorithm is confirmed to work by a simple simulation.},
	booktitle = {Proceedings of the 9th {International} {Conference} on {Neural} {Information} {Processing}, 2002. {ICONIP} '02.},
	author = {Akaho, S.},
	month = nov,
	year = {2002},
	note = {ISSN: null},
	keywords = {Aerospace industry, Constraint optimization, Foot, Iterative algorithms, Neuroscience, Pattern recognition, Quadratic programming, Space technology, Support vector machine classification, Support vector machines, input space, iterative methods, iterative solution, margin maximization, parameter initialization, quadratic programming, quadratic programming problem, support vector machines},
	pages = {1069--1073 vol.2},
}
